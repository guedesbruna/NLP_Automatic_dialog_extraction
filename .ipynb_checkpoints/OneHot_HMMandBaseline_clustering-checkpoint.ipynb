{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "#sklearn imports\n",
    "from sklearn.decomposition import PCA #Principal Component Analysis\n",
    "from sklearn.manifold import TSNE #T-Distributed Stochastic Neighbor Embedding\n",
    "from sklearn.cluster import KMeans #K-Means Clustering\n",
    "from sklearn.preprocessing import StandardScaler #used for 'Feature Scaling'\n",
    "\n",
    "import ast\n",
    "\n",
    "from nltk import ngrams\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import scipy.cluster.hierarchy as sch # for dendogram\n",
    "\n",
    "#sklearn imports\n",
    "from sklearn.decomposition import PCA #Principal Component Analysis\n",
    "from sklearn.manifold import TSNE #T-Distributed Stochastic Neighbor Embedding\n",
    "from sklearn.cluster import KMeans #K-Means Clustering\n",
    "from sklearn.preprocessing import StandardScaler #used for 'Feature Scaling'\n",
    "\n",
    "#plotly imports\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data_TM2/processed/processed_utterances_sentence_DA_labeling.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create input in correct format (one hot) for clustering later\n",
    "\n",
    "#### First identify all possible unigrams, bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_DA = []\n",
    "for row in range(len(df['all_DA'])):\n",
    "    inst = ast.literal_eval(df['all_DA'][row])\n",
    "    inst = [i.strip(\"[]\") for i in inst]\n",
    "    all_DA.append(inst)\n",
    "    \n",
    "df['all_DA'] = all_DA\n",
    "\n",
    "df.drop(df.iloc[:, 8:-1], inplace = True, axis = 1)\n",
    "df = df.explode('all_DA')\n",
    "df['all_DA'] = df['all_DA'].fillna('<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = list(df['all_DA'].unique())\n",
    "one_hot_ubtf = []\n",
    "\n",
    "#uni\n",
    "[one_hot_ubtf.append([x]) for x in unique]\n",
    "one_hot_ubtf\n",
    "\n",
    "#bi\n",
    "for i in range(len(unique)):\n",
    "    for j in range(len(unique)):\n",
    "        one_hot_ubtf.append([unique[i], unique[j]])\n",
    "        \n",
    "#tri >>>>>> until here 12,719\n",
    "for i in range(len(unique)):\n",
    "    for j in range(len(unique)):\n",
    "        for k in range(len(unique)):\n",
    "            one_hot_ubtf.append([unique[i], unique[j], unique[k]])\n",
    "            \n",
    "# #four >>>>>>> from here 292,560 variables\n",
    "# for i in range(len(unique)):\n",
    "#     for j in range(len(unique)):\n",
    "#         for k in range(len(unique)):\n",
    "#             for l in range(len(unique)):\n",
    "#                 one_hot_ubtf.append([unique[i], unique[j], unique[k], unique[l]])\n",
    "\n",
    "len(one_hot_ubtf)\n",
    "\n",
    "with open('./src/generated_files/dict_all_possible_grams.pkl', 'wb') as fp:\n",
    "    pickle.dump(one_hot_ubtf, fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now get patterns identified by HMM or baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### now use results from HMM, i.e the detected patterns to make a one hot. \n",
    "# list os lists, each sublist is one pattern found in the HMM\n",
    "\n",
    "a_file = open('./src/generated_files/sorted_dict_HMM.pkl', \"rb\") #'./src/generated_files/hmm_results.pkl'\n",
    "hmm_results = pickle.load(a_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_patterns = []\n",
    "for key in hmm_results.keys():\n",
    "    hmm_patterns.append(key.split())\n",
    "    \n",
    "hmm_patterns = hmm_patterns[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now make uni, bi, tri found in each pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25140\n",
      "25126\n"
     ]
    }
   ],
   "source": [
    "print(len(hmm_patterns))\n",
    "\n",
    "#remove unigram patterns found in hmm, since they can't be considered patterns\n",
    "for pat in hmm_patterns:\n",
    "    if len(pat) == 1:\n",
    "        hmm_patterns.remove(pat)\n",
    "        \n",
    "print(len(hmm_patterns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_grams = {}\n",
    "\n",
    "for e, pat in enumerate(hmm_patterns):\n",
    "    pat = (' ').join(pat)\n",
    "    \n",
    "    for uni in ngrams(pat.split(), 1):\n",
    "        uni = list(uni)\n",
    "        if pat not in dict_grams:\n",
    "            dict_grams[pat] = [uni]\n",
    "        else:\n",
    "            dict_grams[pat].extend([uni])\n",
    "        \n",
    "    for bi in ngrams(pat.split(), 2):\n",
    "        bi = list(bi)\n",
    "        if pat not in dict_grams:\n",
    "            dict_grams[pat] = [bi]\n",
    "        dict_grams[pat].extend([bi])\n",
    "\n",
    "    for tri in ngrams(pat.split(), 3):\n",
    "        tri = list(tri)\n",
    "        if pat not in dict_grams:\n",
    "            dict_grams[pat] = [tri]\n",
    "        dict_grams[pat].extend([tri])\n",
    "\n",
    "# dict_grams\n",
    "\n",
    "# #sanity check\n",
    "# len(hmm_patterns) == len(dict_grams.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create matrix of zeros and populate it with 1 if one of the values of each key is found (i.e a uni,bi or tri per pattern)\n",
    "matrix >> rows are patterns and columns are all possible uni, bi, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create matrix size len(one_hot_ubtf) X len(hmm_results)\n",
    "one_hot_mat = np.zeros((len(dict_grams.keys()), len(one_hot_ubtf))) #format: mat_one_hot[row=dict_grams.keys][column_one_hot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one_hot_mat_exemplo = np.zeros((len(dict_grams_exemplo.keys()), len(one_hot_ubtf_exemplo)))\n",
    "# one_hot_ubtf_exemplo = [['U_answer'], ['NADA2'],  ['A_detail_request'], ['U_answer', 'A_detail_request'],\n",
    "#                        ['A_completion_check'], ['A_completion_check', 'A_detail_request'], ['NADA1']]\n",
    "# dict_grams_exemplo = {'ex0':[['U_answer'], ['A_detail_request'], ['U_answer', 'A_detail_request']],\n",
    "#                       'ex1':[['A_completion_check'], ['A_detail_request'], ['A_completion_check', 'A_detail_request']]}\n",
    "\n",
    "\n",
    "for e, pattern in enumerate(dict_grams.values()):\n",
    "    for g, pat in enumerate(pattern):\n",
    "#         print(g, pat)\n",
    "        for f, pos_ngr in enumerate(one_hot_ubtf):\n",
    "            if pos_ngr == pat:\n",
    "                one_hot_mat[e][f] = 1\n",
    "\n",
    "one_hot_mat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check to see if there are at least same number of 1s then pattern. (pattern can be bigger cause DA can repeat, but never smaller)\n",
    "values = dict_grams.values()\n",
    "values = list(values)\n",
    "\n",
    "pat_n = 10000\n",
    "one_hot_mat[pat_n].sum() <= (len(values[pat_n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./src/generated_files/one_hot_mat_HMM.pkl', 'wb') as fp:\n",
    "    pickle.dump(one_hot_mat, fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select adequate number for K\n",
    "TWO METHODS: \n",
    "   - sum of squared distances then elbow plot\n",
    "   https://blog.cambridgespark.com/how-to-determine-the-optimal-number-of-clusters-for-k-means-clustering-14f27070048f \n",
    "\n",
    "   - Sillhouette: in the link code of how to plot sillhouette \n",
    "   https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py\n",
    "   \n",
    "   - Dendogram: I won't be able to see anything relevant with this many examples (25k)\n",
    "       - dendrogram = sch.dendrogram(sch.linkage(one_hot_mat, method='ward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sum_of_squared_distances = []\n",
    "sillhouete_dict = {}\n",
    "\n",
    "K = range(2,20)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(one_hot_mat[:5000])\n",
    "    cluster_labels = km.fit_predict(one_hot_mat[:5000])\n",
    "    Sum_of_squared_distances.append(km.inertia_) # from K_means > inertia_: float: Sum of squared distances of samples to their closest cluster center, weighted by the sample weights if provided.\n",
    "    silhouette_avg = silhouette_score(one_hot_mat[:5000], cluster_labels)\n",
    "    sillhouete_dict[k] = silhouette_avg\n",
    "    print(\"For n_clusters =\", k,\"The average silhouette_score is :\",silhouette_avg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./src/generated_files/sillhouete_dict.pkl', 'wb') as fp:\n",
    "    pickle.dump(sillhouete_dict, fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "\n",
    "plt.savefig('./src/generated_files/Optimal_k_for_kmeans.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make choice of K based on results above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize our model with correct number of clusters:\n",
    "\n",
    "kmeans = KMeans(n_clusters=7)\n",
    "\n",
    "#Fit our model\n",
    "kmeans.fit(one_hot_mat)\n",
    "\n",
    "#Find which cluster each data-point belongs to\n",
    "clusters = kmeans.predict(one_hot_mat)\n",
    "\n",
    "# #Add the cluster vector to our DataFrame, X\n",
    "# # X[\"Cluster\"] = clusters\n",
    "\n",
    "len(clusters) == len(one_hot_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = pd.DataFrame(one_hot_mat)\n",
    "plot_df['Cluster'] = list(clusters)\n",
    "plot_df['patterns'] = list(dict_grams.keys())\n",
    "\n",
    "with open('./src/generated_files/df_one_hot_with_cluster_assignment.pkl', 'wb') as fp:\n",
    "    pickle.dump(plot_df, fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_file = open(\"./src/generated_files/df_one_hot_with_cluster_assignment.pkl\", \"rb\")\n",
    "plot_df = pickle.load(a_file)\n",
    "\n",
    "# plot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampling \n",
    "#plotX is a DataFrame containing 5000 values sampled randomly from X\n",
    "plot_df_x = pd.DataFrame(np.array(plot_df.sample(1000)))\n",
    "\n",
    "#Rename plotX's columns since it was briefly converted to an np.array above\n",
    "# plot_df.columns = df_to_cluster.columns\n",
    "\n",
    "plot_df_x.head()\n",
    "plot_df_x = plot_df_x.rename(columns={12719:'Cluster', 12720:'patterns'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE\n",
    "code from notebook in Dropbox\n",
    "\n",
    "Explanation on how to analyse it:\n",
    "https://distill.pub/2016/misread-tsne/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set our perplexity\n",
    "perplexity = 50\n",
    "\n",
    "#T-SNE with two dimensions\n",
    "tsne_1d = TSNE(n_components=1, perplexity=perplexity)\n",
    "\n",
    "#T-SNE with two dimensions\n",
    "tsne_2d = TSNE(n_components=2, perplexity=perplexity)\n",
    "\n",
    "# #T-SNE with three dimensions\n",
    "# tsne_3d = TSNE(n_components=3, perplexity=perplexity)\n",
    "\n",
    "#This DataFrame holds a single dimension,built by T-SNE\n",
    "# TCs_1d = pd.DataFrame(tsne_1d.fit_transform(plot_df_x.drop([\"Cluster\", \"patterns\"], axis=1)))\n",
    "\n",
    "#This DataFrame contains two dimensions, built by T-SNE\n",
    "TCs_2d = pd.DataFrame(tsne_2d.fit_transform(plot_df_x.drop([\"Cluster\", \"patterns\"], axis=1)))\n",
    "\n",
    "\n",
    "# TCs_1d.columns = [\"TC1_1d\"]\n",
    "\n",
    "\n",
    "# #And this DataFrame contains three dimensions, built by T-SNE\n",
    "# TCs_3d = pd.DataFrame(tsne_3d.fit_transform(df_to_cluster.drop([\"Cluster\"], axis=1)))\n",
    "\n",
    "#\"TC1_2d\" means: 'The first component of the components created for 2-D visualization, by T-SNE.'\n",
    "#And \"TC2_2d\" means: 'The second component of the components created for 2-D visualization, by T-SNE.'\n",
    "TCs_2d.columns = [\"TC1_2d\",\"TC2_2d\"]\n",
    "\n",
    "# TCs_3d.columns = [\"TC1_3d\",\"TC2_3d\",\"TC3_3d\"]\n",
    "\n",
    "plot_df_x = pd.concat([plot_df_x,TCs_1d,TCs_2d], axis=1, join='inner')\n",
    "\n",
    "#if plotting one d visualization create also a dummy variable\n",
    "# plot_df_x[\"dummy\"] = 0\n",
    "\n",
    "#Each of these new DataFrames will hold all of the values contained in exacltly one of the clusters. For example, all of the values contained within the DataFrame, cluster0 will belong to 'cluster 0', and all the values contained in DataFrame, cluster1 will belong to 'cluster 1', etc.\n",
    "\n",
    "cluster0 = plot_df_x[plot_df_x[\"Cluster\"] == 0]\n",
    "cluster1 = plot_df_x[plot_df_x[\"Cluster\"] == 1]\n",
    "cluster2 = plot_df_x[plot_df_x[\"Cluster\"] == 2]\n",
    "cluster3 = plot_df_x[plot_df_x[\"Cluster\"] == 3]\n",
    "cluster4 = plot_df_x[plot_df_x[\"Cluster\"] == 4]\n",
    "cluster5 = plot_df_x[plot_df_x[\"Cluster\"] == 5]\n",
    "cluster6 = plot_df_x[plot_df_x[\"Cluster\"] == 6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instructions for building the 2-D plot\n",
    "\n",
    "#trace0 is for 'Cluster 0'\n",
    "trace0 = go.Scatter(\n",
    "                    x = cluster0[\"TC1_2d\"],\n",
    "                    y = cluster0[\"TC2_2d\"],\n",
    "                    mode = \"markers\",\n",
    "                    name = \"Cluster 0\",\n",
    "                    marker = dict(color = 'rgba(255, 128, 255, 0.8)'),\n",
    "                    text = None)\n",
    "\n",
    "#trace1 is for 'Cluster 1'\n",
    "trace1 = go.Scatter(\n",
    "                    x = cluster1[\"TC1_2d\"],\n",
    "                    y = cluster1[\"TC2_2d\"],\n",
    "                    mode = \"markers\",\n",
    "                    name = \"Cluster 1\",\n",
    "                    marker = dict(color = 'rgba(255, 128, 2, 0.8)'),\n",
    "                    text = None)\n",
    "\n",
    "#trace2 is for 'Cluster 2'\n",
    "trace2 = go.Scatter(\n",
    "                    x = cluster2[\"TC1_2d\"],\n",
    "                    y = cluster2[\"TC2_2d\"],\n",
    "                    mode = \"markers\",\n",
    "                    name = \"Cluster 2\",\n",
    "                    marker = dict(color = 'rgba(0, 255, 10, 0.8)'),\n",
    "                    text = None)\n",
    "\n",
    "#trace3 is for 'Cluster 3'\n",
    "trace3 = go.Scatter(\n",
    "                    x = cluster3[\"TC1_2d\"],\n",
    "                    y = cluster3[\"TC2_2d\"],\n",
    "                    mode = \"markers\",\n",
    "                    name = \"Cluster 3\",\n",
    "                    marker = dict(color = 'rgba(0, 0, 255, 0.8)'),\n",
    "                    text = None)\n",
    "\n",
    "#trace4 is for 'Cluster 4'\n",
    "trace4 = go.Scatter(\n",
    "                    x = cluster4[\"TC1_2d\"],\n",
    "                    y = cluster4[\"TC2_2d\"],\n",
    "                    mode = \"markers\",\n",
    "                    name = \"Cluster 4\",\n",
    "                    marker = dict(color = 'rgba(106,90,205, 0.8)'),\n",
    "                    text = None)\n",
    "\n",
    "#trace5 is for 'Cluster 5'\n",
    "trace5 = go.Scatter(\n",
    "                    x = cluster5[\"TC1_2d\"],\n",
    "                    y = cluster5[\"TC2_2d\"],\n",
    "                    mode = \"markers\",\n",
    "                    name = \"Cluster 5\",\n",
    "                    marker = dict(color = 'rgba(255,0,0, 0.8)'),\n",
    "                    text = None)\n",
    "\n",
    "#trace6 is for 'Cluster 6'\n",
    "trace6 = go.Scatter(\n",
    "                    x = cluster6[\"TC1_2d\"],\n",
    "                    y = cluster6[\"TC2_2d\"],\n",
    "                    mode = \"markers\",\n",
    "                    name = \"Cluster 6\",\n",
    "                    marker = dict(color = 'rgba(60,179,113, 0.8)'),\n",
    "                    text = None)\n",
    "\n",
    "\n",
    "\n",
    "data = [trace0, trace1, trace2, trace3, trace4, trace5, trace6]\n",
    "\n",
    "title = \"Visualizing Clusters in Two Dimensions Using T-SNE (perplexity=\" + str(perplexity) + \")\"\n",
    "\n",
    "layout = dict(title = title,\n",
    "              xaxis= dict(title= 'TC1',ticklen= 5,zeroline= False),\n",
    "              yaxis= dict(title= 'TC2',ticklen= 5,zeroline= False)\n",
    "             )\n",
    "\n",
    "fig = dict(data = data, layout = layout)\n",
    "\n",
    "iplot(fig)\n",
    "\n",
    "py.offline.plot(fig, filename=\"./src/generated_files/t-SNE_visualization.png\")\n",
    "# IPython.display.display(IPython.display.IFrame(src=\"./src/generated_files/t_SNE_visualization.png\", width=1200, height=800))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA with one principal component\n",
    "pca_1d = PCA(n_components=1)\n",
    "\n",
    "#PCA with two principal components\n",
    "pca_2d = PCA(n_components=2)\n",
    "\n",
    "\n",
    "#This DataFrame holds a single dimension,built by T-SNE\n",
    "PCs_1d = pd.DataFrame(pca_1d.fit_transform(plot_df_x.drop([\"Cluster\", \"patterns\"], axis=1)))\n",
    "\n",
    "#This DataFrame contains two dimensions, built by T-SNE\n",
    "PCs_2d = pd.DataFrame(pca_2d.fit_transform(plot_df_x.drop([\"Cluster\", \"patterns\"], axis=1)))\n",
    "\n",
    "PCs_1d.columns = [\"PC1_1d\"]\n",
    "\n",
    "#\"PC1_2d\" means: 'The first principal component of the components created for 2-D visualization, by PCA.'\n",
    "#And \"PC2_2d\" means: 'The second principal component of the components created for 2-D visualization, by PCA.'\n",
    "PCs_2d.columns = [\"PC1_2d\", \"PC2_2d\"]\n",
    "\n",
    "\n",
    "plot_df_x = pd.concat([plot_df_x,PCs_1d,PCs_2d], axis=1, join='inner')\n",
    "\n",
    "#if plotting one d visualization create also a dummy variable\n",
    "# plot_df_x[\"dummy\"] = 0\n",
    "\n",
    "#Each of these new DataFrames will hold all of the values contained in exacltly one of the clusters. For example, all of the values contained within the DataFrame, cluster0 will belong to 'cluster 0', and all the values contained in DataFrame, cluster1 will belong to 'cluster 1', etc.\n",
    "\n",
    "cluster0 = plot_df_x[plot_df_x[\"Cluster\"] == 0]\n",
    "cluster1 = plot_df_x[plot_df_x[\"Cluster\"] == 1]\n",
    "cluster2 = plot_df_x[plot_df_x[\"Cluster\"] == 2]\n",
    "cluster3 = plot_df_x[plot_df_x[\"Cluster\"] == 3]\n",
    "cluster4 = plot_df_x[plot_df_x[\"Cluster\"] == 4]\n",
    "cluster5 = plot_df_x[plot_df_x[\"Cluster\"] == 5]\n",
    "cluster6 = plot_df_x[plot_df_x[\"Cluster\"] == 6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instructions for building the 2-D plot\n",
    "\n",
    "#trace0 is for 'Cluster 0'\n",
    "trace0 = go.Scatter(\n",
    "                    x = cluster0[\"TC1_2d\"],\n",
    "                    y = cluster0[\"TC2_2d\"],\n",
    "                    mode = \"markers\",\n",
    "                    name = \"Cluster 0\",\n",
    "                    marker = dict(color = 'rgba(255, 128, 255, 0.8)'),\n",
    "                    text = None)\n",
    "\n",
    "#trace1 is for 'Cluster 1'\n",
    "trace1 = go.Scatter(\n",
    "                    x = cluster1[\"PC1_2d\"],\n",
    "                    y = cluster1[\"PC2_2d\"],\n",
    "                    mode = \"markers\",\n",
    "                    name = \"Cluster 1\",\n",
    "                    marker = dict(color = 'rgba(255, 128, 2, 0.8)'),\n",
    "                    text = None)\n",
    "\n",
    "#trace2 is for 'Cluster 2'\n",
    "trace2 = go.Scatter(\n",
    "                    x = cluster2[\"PC1_2d\"],\n",
    "                    y = cluster2[\"PC2_2d\"],\n",
    "                    mode = \"markers\",\n",
    "                    name = \"Cluster 2\",\n",
    "                    marker = dict(color = 'rgba(0, 255, 10, 0.8)'),\n",
    "                    text = None)\n",
    "\n",
    "#trace3 is for 'Cluster 3'\n",
    "trace3 = go.Scatter(\n",
    "                    x = cluster3[\"PC1_2d\"],\n",
    "                    y = cluster3[\"PC2_2d\"],\n",
    "                    mode = \"markers\",\n",
    "                    name = \"Cluster 3\",\n",
    "                    marker = dict(color = 'rgba(0, 0, 255, 0.8)'),\n",
    "                    text = None)\n",
    "\n",
    "#trace4 is for 'Cluster 4'\n",
    "trace4 = go.Scatter(\n",
    "                    x = cluster4[\"PC1_2d\"],\n",
    "                    y = cluster4[\"PC2_2d\"],\n",
    "                    mode = \"markers\",\n",
    "                    name = \"Cluster 4\",\n",
    "                    marker = dict(color = 'rgba(106,90,205, 0.8)'),\n",
    "                    text = None)\n",
    "\n",
    "#trace5 is for 'Cluster 5'\n",
    "trace5 = go.Scatter(\n",
    "                    x = cluster5[\"PC1_2d\"],\n",
    "                    y = cluster5[\"PC2_2d\"],\n",
    "                    mode = \"markers\",\n",
    "                    name = \"Cluster 5\",\n",
    "                    marker = dict(color = 'rgba(255,0,0, 0.8)'),\n",
    "                    text = None)\n",
    "\n",
    "#trace6 is for 'Cluster 6'\n",
    "trace6 = go.Scatter(\n",
    "                    x = cluster6[\"PC1_2d\"],\n",
    "                    y = cluster6[\"PC2_2d\"],\n",
    "                    mode = \"markers\",\n",
    "                    name = \"Cluster 6\",\n",
    "                    marker = dict(color = 'rgba(60,179,113, 0.8)'),\n",
    "                    text = None)\n",
    "\n",
    "\n",
    "\n",
    "data = [trace0, trace1, trace2, trace3, trace4, trace5, trace6]\n",
    "\n",
    "title = \"Visualizing Clusters in Two Dimensions Using PCA\"\n",
    "\n",
    "layout = dict(title = title,\n",
    "              xaxis= dict(title= 'PC1',ticklen= 5,zeroline= False),\n",
    "              yaxis= dict(title= 'PC2',ticklen= 5,zeroline= False)\n",
    "             )\n",
    "\n",
    "fig = dict(data = data, layout = layout)\n",
    "\n",
    "iplot(fig)\n",
    "\n",
    "py.offline.plot(fig, filename=\"./src/generated_files/PCA_visualization.png\")\n",
    "# IPython.display.display(IPython.display.IFrame(src=\"./src/generated_files/PCA_visualization.png\", width=1200, height=800))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
