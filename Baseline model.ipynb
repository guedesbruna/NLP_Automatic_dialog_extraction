{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40ac04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "#Basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#sklearn imports\n",
    "from sklearn.decomposition import PCA #Principal Component Analysis\n",
    "from sklearn.manifold import TSNE #T-Distributed Stochastic Neighbor Embedding\n",
    "from sklearn.cluster import KMeans #K-Means Clustering\n",
    "from sklearn.preprocessing import StandardScaler #used for 'Feature Scaling'\n",
    "\n",
    "#plotly imports\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508914ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data_TM2/synt_annotated_data.csv', index_col=0)\n",
    "# df['concat_da_names'] = df.iloc[:, 8:21].apply(lambda row: row.dropna().tolist(), axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cc043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_cluster = df.iloc[:, 8:21]\n",
    "df_to_cluster.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bea2f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "DA = ['repair_initiator','greeting','request_summary','confirmation','receipt','disconfirmation',\n",
    "      'sequence_closer','completion_check','hold_request','partial_request', 'detail_request', 'grant', 'answer'] \n",
    "\n",
    "for e,i in enumerate(DA):\n",
    "    print(e,i, e+8) #(class number, original tag, column number)\n",
    "    df_to_cluster.iloc[:,e] = df_to_cluster.iloc[:,e].replace(i, 1)\n",
    "    df_to_cluster.iloc[:,e] = df_to_cluster.iloc[:,e].fillna(0)\n",
    "    \n",
    "df_to_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5da8af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select only labeled dataset\n",
    "\n",
    "df.dropna(subset=['DA_rep_init', 'DA_greet', 'DA_req_sum',\n",
    "       'DA_conf', 'DA_receipt', 'DA_disconf', 'DA_closer', 'DA_comp_check',\n",
    "       'DA_hold', 'DA_partial_req', 'DA_detail_req', 'DA_grant', 'DA_answer'], how='all', inplace = True)\n",
    "\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d0eff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DA = ['repair_initiator','greeting','request_summary','confirmation','receipt','disconfirmation',\n",
    "      'sequence_closer','completion_check','hold_request','partial_request', 'detail_request', 'grant', 'answer'] \n",
    "\n",
    "#replace tags per number of class\n",
    "for e,i in enumerate(DA):\n",
    "    print(e,i, e+8) #(class number, original tag, column number)\n",
    "    df.iloc[:,e+8] = df.iloc[:,e+8].replace(i, (e+1))\n",
    "    df.iloc[:,e+8] = df.iloc[:,e+8].fillna(0)\n",
    "\n",
    "#     df.iloc[:,e+8] =df.iloc[:,e+8].astype(int)\n",
    "    \n",
    "df['concat_col'] = df.iloc[:, 8:21].apply(lambda row: row.dropna().tolist(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd319307",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_col = []\n",
    "for row in df['concat_col']:\n",
    "    row = [int(x) for x in row]\n",
    "    concat_col.append(row)\n",
    "    \n",
    "df['concat_col'] = concat_col\n",
    "# print(df['concat_col'].head())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d121ad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filled with zeros\n",
    "X = df['concat_col'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd10ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "\n",
    "X = [[0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0],\n",
    "      [0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0],\n",
    "      [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0],\n",
    "      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13],\n",
    "      [0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 11, 0, 0],\n",
    "      [0, 0, 0, 4, 0, 0, 7, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 3, 4, 0, 0, 7, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 4, 0, 0, 7, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0],\n",
    "      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 12, 0],\n",
    "      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13],\n",
    "      [0, 0, 0, 4, 0, 0, 7, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 4, 0, 0, 7, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0]]\n",
    "\n",
    "\n",
    "# X = [[10,20],[5,15],[13,10],[20,10],[1,13],[2,12]]\n",
    "\n",
    "dendrogram = sch.dendrogram(sch.linkage(X, method='ward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae189631",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sns.clustermap(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986fa5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\n",
    "model.fit(X)\n",
    "labels = model.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bdb040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(X[labels==0,0], X[labels==0,1], s=50, marker='o', color='red')\n",
    "# plt.scatter(X[labels==1, 0], X[labels==1, 1], s=50, marker='o', color='blue')\n",
    "# plt.scatter(X[labels==2, 0], X[labels==2, 1], s=50, marker='o', color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5357eb1a",
   "metadata": {},
   "source": [
    "## Notebook t-sne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d84cf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to build our clusters.\n",
    "# In this kernel, we will be visualizing only three different clusters on our data. \n",
    "#I chose three because I found it to be a good number of clusters to help us visualize our data\n",
    "\n",
    "\n",
    "########### en lugar de kmean use HAC ##########\n",
    "\n",
    "\n",
    "#Initialize our model\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "#Fit our model\n",
    "kmeans.fit(df_to_cluster)\n",
    "KMeans(n_clusters=3)\n",
    "#Find which cluster each data-point belongs to\n",
    "clusters = kmeans.predict(df_to_cluster)\n",
    "#Add the cluster vector to our DataFrame, X\n",
    "df_to_cluster[\"Cluster\"] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efb481b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampling \n",
    "#plotX is a DataFrame containing 5000 values sampled randomly from X\n",
    "plot_df = pd.DataFrame(np.array(df_to_cluster.sample(5000)))\n",
    "\n",
    "#Rename plotX's columns since it was briefly converted to an np.array above\n",
    "plot_df.columns = df_to_cluster.columns\n",
    "\n",
    "# plot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b45791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set our perplexity\n",
    "perplexity = 50\n",
    "\n",
    "#T-SNE with two dimensions\n",
    "tsne_2d = TSNE(n_components=2, perplexity=perplexity)\n",
    "\n",
    "# #T-SNE with three dimensions\n",
    "# tsne_3d = TSNE(n_components=3, perplexity=perplexity)\n",
    "\n",
    "#This DataFrame contains two dimensions, built by T-SNE\n",
    "TCs_2d = pd.DataFrame(tsne_2d.fit_transform(plot_df.drop([\"Cluster\"], axis=1)))\n",
    "\n",
    "# #And this DataFrame contains three dimensions, built by T-SNE\n",
    "# TCs_3d = pd.DataFrame(tsne_3d.fit_transform(df_to_cluster.drop([\"Cluster\"], axis=1)))\n",
    "\n",
    "#\"TC1_2d\" means: 'The first component of the components created for 2-D visualization, by T-SNE.'\n",
    "#And \"TC2_2d\" means: 'The second component of the components created for 2-D visualization, by T-SNE.'\n",
    "TCs_2d.columns = [\"TC1_2d\",\"TC2_2d\"]\n",
    "\n",
    "# TCs_3d.columns = [\"TC1_3d\",\"TC2_3d\",\"TC3_3d\"]\n",
    "\n",
    "df_to_cluster = pd.concat([plot_df,TCs_1d,TCs_2d,TCs_3d], axis=1, join='inner')\n",
    "\n",
    "\n",
    "#Each of these new DataFrames will hold all of the values contained in exacltly one of the clusters. For example, all of the values contained within the DataFrame, cluster0 will belong to 'cluster 0', and all the values contained in DataFrame, cluster1 will belong to 'cluster 1', etc.\n",
    "\n",
    "cluster0 = plot_df[plot_df[\"Cluster\"] == 0]\n",
    "cluster1 = plot_df[plot_df[\"Cluster\"] == 1]\n",
    "cluster2 = plot_df[plot_df[\"Cluster\"] == 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b68c5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instructions for building the 2-D plot\n",
    "\n",
    "#trace1 is for 'Cluster 0'\n",
    "trace1 = go.Scatter(\n",
    "                    x = cluster0[\"TC1_2d\"],\n",
    "                    y = cluster0[\"TC2_2d\"],\n",
    "                    mode = \"markers\",\n",
    "                    name = \"Cluster 0\",\n",
    "                    marker = dict(color = 'rgba(255, 128, 255, 0.8)'),\n",
    "                    text = None)\n",
    "\n",
    "#trace2 is for 'Cluster 1'\n",
    "trace2 = go.Scatter(\n",
    "                    x = cluster1[\"TC1_2d\"],\n",
    "                    y = cluster1[\"TC2_2d\"],\n",
    "                    mode = \"markers\",\n",
    "                    name = \"Cluster 1\",\n",
    "                    marker = dict(color = 'rgba(255, 128, 2, 0.8)'),\n",
    "                    text = None)\n",
    "\n",
    "#trace3 is for 'Cluster 2'\n",
    "trace3 = go.Scatter(\n",
    "                    x = cluster2[\"TC1_2d\"],\n",
    "                    y = cluster2[\"TC2_2d\"],\n",
    "                    mode = \"markers\",\n",
    "                    name = \"Cluster 2\",\n",
    "                    marker = dict(color = 'rgba(0, 255, 200, 0.8)'),\n",
    "                    text = None)\n",
    "\n",
    "data = [trace1, trace2, trace3]\n",
    "\n",
    "title = \"Visualizing Clusters in Two Dimensions Using T-SNE (perplexity=\" + str(perplexity) + \")\"\n",
    "\n",
    "layout = dict(title = title,\n",
    "              xaxis= dict(title= 'TC1',ticklen= 5,zeroline= False),\n",
    "              yaxis= dict(title= 'TC2',ticklen= 5,zeroline= False)\n",
    "             )\n",
    "\n",
    "fig = dict(data = data, layout = layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0fb5be",
   "metadata": {},
   "source": [
    "## N-grams from Jurafsky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca167e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "025853fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data_TM2/processed_utterances_sentence_DA_labeling.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a7dd96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "####### correct format of column that was string and should be list\n",
    "\n",
    "import ast\n",
    "\n",
    "print(type(df['all_DA'][0]))\n",
    "all_DA = []\n",
    "for row in range(len(df['all_DA'])):\n",
    "    inst = ast.literal_eval(df['all_DA'][row])\n",
    "    inst = [i.strip(\"[]\") for i in inst]\n",
    "    all_DA.append(inst)\n",
    "    \n",
    "df['all_DA'] = all_DA\n",
    "\n",
    "print(type(df['all_DA'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04ce4e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        U_greeting\n",
       "1             <UNK>\n",
       "2        A_greeting\n",
       "3    A_confirmation\n",
       "4        U_greeting\n",
       "Name: all_DA, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(df.iloc[:, 8:-1], inplace = True, axis = 1)\n",
    "df = df.explode('all_DA')\n",
    "df['all_DA'] = df['all_DA'].fillna('<UNK>')\n",
    "df['all_DA'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb892e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #adding unknown token <UNK> for unlabeled utterances and *(removing more than 1 label for each utterance)\n",
    "\n",
    "# ALL_DA = []\n",
    "# for row in range(len(df['all_DA'])):\n",
    "#     if len(df['all_DA'].iloc[row]) == 0:\n",
    "#         ALL_DA.append(['<UNK>'])\n",
    "# #     *uncomment the below to leave only one label per utterance. Otherwise multiple labels may be used \n",
    "# #     elif len(df['all_DA'].iloc[row]) >1:\n",
    "# #         ALL_DA.append([df['all_DA'].iloc[row][0]])\n",
    "#     else:\n",
    "#         ALL_DA.append(df['all_DA'].iloc[row])\n",
    "        \n",
    "# # ALL_DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa0a206f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'U_greeting',\n",
       " '<UNK>',\n",
       " 'A_greeting',\n",
       " 'A_confirmation',\n",
       " 'U_greeting',\n",
       " '<UNK>',\n",
       " '<UNK>',\n",
       " 'A_greeting',\n",
       " '<UNK>',\n",
       " '<UNK>',\n",
       " '<UNK>',\n",
       " '<UNK>',\n",
       " 'A_confirmation',\n",
       " 'A_confirmation',\n",
       " 'A_sequence_closer',\n",
       " '<UNK>',\n",
       " '<UNK>',\n",
       " '<UNK>',\n",
       " '<UNK>',\n",
       " 'A_request_summary',\n",
       " 'A_confirmation',\n",
       " 'A_sequence_closer',\n",
       " 'U_confirmation',\n",
       " 'A_confirmation',\n",
       " 'A_sequence_closer',\n",
       " '<UNK>',\n",
       " 'A_sequence_closer',\n",
       " '<UNK>',\n",
       " '<UNK>',\n",
       " '<UNK>',\n",
       " '<UNK>',\n",
       " 'A_confirmation',\n",
       " 'A_sequence_closer',\n",
       " '<UNK>',\n",
       " '<UNK>',\n",
       " '<UNK>',\n",
       " 'A_confirmation',\n",
       " 'A_sequence_closer',\n",
       " 'U_sequence_closer',\n",
       " 'U_sequence_closer',\n",
       " 'A_confirmation',\n",
       " 'A_sequence_closer',\n",
       " 'A_sequence_closer',\n",
       " '<UNK>',\n",
       " 'U_greeting',\n",
       " 'U_sequence_closer',\n",
       " '<e>']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#solution for all dialogues. After This data will be prepared to serve as input\n",
    "\n",
    "unique_ids = df['conversation_id'].unique()\n",
    "\n",
    "start = '<s>'\n",
    "end ='<e>'\n",
    "full_DA = []  \n",
    "\n",
    "for dialog in unique_ids:\n",
    "    dialog_DA = []   \n",
    "    temp = df.loc[df['conversation_id'] == dialog]\n",
    "    for x in range(len(temp)):\n",
    "        dialog_DA.append(temp['all_DA'].iloc[x])\n",
    "\n",
    "    #insert begin and end tokens for each dialog. This is for bigram only. for tri would need two symbols in the begin and 2 in the end\n",
    "    dialog_DA.insert(0, start)\n",
    "    dialog_DA.append(end)\n",
    "    full_DA.append(dialog_DA)\n",
    "\n",
    "full_DA[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f3502df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #for one example of dialog\n",
    "# start = ['<s>']\n",
    "# end =['<e>']\n",
    "\n",
    "# dialog_DA = []   \n",
    "# temp = df.loc[df['conversation_id'] == 'dlg-00100680-00e0-40fe-8321-6d81b21bfc4f']\n",
    "# [dialog_DA.append(temp['all_DA'][x]) for x in range(len(temp))]\n",
    "\n",
    "# #insert begin and end tokens for each dialog. This is for bigram only. for tri would need two symbols in the begin and 2 in the end\n",
    "# dialog_DA.insert(0, start)\n",
    "# dialog_DA.append(end)\n",
    "\n",
    "# dialog_DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "637647f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten nested lists\n",
    "\n",
    "def flatten(t):\n",
    "    # https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-a-list-of-lists\n",
    "    return [item for sublist in t for item in sublist]\n",
    "\n",
    "# df['labels_UNK_and_SINGLE_DA'] = flatten(ALL_DA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75ddbcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common n-grams without add-1 smoothing: \n",
      "\n",
      "Most common unigrams: \n",
      " [('<UNK>', 197778), ('U_sequence_closer', 57516), ('A_greeting', 53345), ('A_sequence_closer', 47482), ('U_confirmation', 47295)]\n",
      "Most common bigrams: \n",
      " [(('U_confirmation', 'U_sequence_closer'), 6), (('<UNK>', 'A_greeting'), 3), (('U_sequence_closer', '<UNK>'), 3), (('U_greeting', 'U_confirmation'), 3), (('A_greeting', 'U_confirmation'), 2)]\n",
      "\n",
      "Most common trigrams: \n",
      " [(('U_confirmation', 'U_sequence_closer', '<UNK>'), 3), (('U_greeting', 'U_confirmation', 'U_sequence_closer'), 3), (('<UNK>', 'A_greeting', 'U_confirmation'), 2), (('A_greeting', 'U_confirmation', 'U_sequence_closer'), 2), (('<s>', 'A_greeting', '<UNK>'), 1)]\n",
      "\n",
      "Most common fourgrams: \n",
      " [(('<UNK>', 'A_greeting', 'U_confirmation', 'U_sequence_closer'), 2), (('U_greeting', 'U_confirmation', 'U_sequence_closer', '<UNK>'), 2), (('<s>', 'A_greeting', '<UNK>', 'A_greeting'), 1), (('A_greeting', '<UNK>', 'A_greeting', 'U_confirmation'), 1), (('A_greeting', 'U_confirmation', 'U_sequence_closer', '<UNK>'), 1)]\n"
     ]
    }
   ],
   "source": [
    "#modeling phase\n",
    "#from this tutorial: https://medium.com/swlh/language-modelling-with-nltk-20eac7e70853#b9bf\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "# from nltk import word_tokenize\n",
    "\n",
    "unigram=[]\n",
    "bigram=[]\n",
    "trigram=[]\n",
    "fourgram=[]\n",
    "tokenized_text = flatten(unigram)\n",
    "\n",
    "#unigram, bigram, trigram, and fourgram models are created\n",
    "for sequence in full_DA:\n",
    "    unigram.append(sequence)\n",
    "\n",
    "bigram.extend(list(ngrams(sequence, 2)))  \n",
    "trigram.extend(list(ngrams(sequence, 3)))\n",
    "fourgram.extend(list(ngrams(sequence, 4)))\n",
    "\n",
    "freq_uni = Counter(flatten(unigram))\n",
    "freq_bi = nltk.FreqDist(bigram)\n",
    "freq_tri = nltk.FreqDist(trigram)\n",
    "freq_four = nltk.FreqDist(fourgram)\n",
    "\n",
    "\n",
    "print(\"Most common n-grams without add-1 smoothing: \\n\")\n",
    "print (\"Most common unigrams: \\n\", freq_uni.most_common(5))\n",
    "print (\"Most common bigrams: \\n\", freq_bi.most_common(5))\n",
    "print (\"\\nMost common trigrams: \\n\", freq_tri.most_common(5))\n",
    "print (\"\\nMost common fourgrams: \\n\", freq_four.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a13954a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', 'A_greeting'),\n",
       " ('A_greeting', '<UNK>'),\n",
       " ('<UNK>', 'A_greeting'),\n",
       " ('A_greeting', 'U_confirmation'),\n",
       " ('U_confirmation', 'U_sequence_closer'),\n",
       " ('U_sequence_closer', '<UNK>'),\n",
       " ('<UNK>', 'A_greeting'),\n",
       " ('A_greeting', 'U_confirmation'),\n",
       " ('U_confirmation', 'U_sequence_closer'),\n",
       " ('U_sequence_closer', 'U_greeting'),\n",
       " ('U_greeting', '<UNK>'),\n",
       " ('<UNK>', 'A_greeting'),\n",
       " ('A_greeting', 'A_greeting'),\n",
       " ('A_greeting', 'U_greeting'),\n",
       " ('U_greeting', 'U_confirmation'),\n",
       " ('U_confirmation', 'U_sequence_closer'),\n",
       " ('U_sequence_closer', 'A_hold_request'),\n",
       " ('A_hold_request', 'A_greeting'),\n",
       " ('A_greeting', 'A_confirmation'),\n",
       " ('A_confirmation', 'U_greeting'),\n",
       " ('U_greeting', 'U_confirmation'),\n",
       " ('U_confirmation', 'U_sequence_closer'),\n",
       " ('U_sequence_closer', '<UNK>'),\n",
       " ('<UNK>', 'U_greeting'),\n",
       " ('U_greeting', 'U_confirmation'),\n",
       " ('U_confirmation', 'U_sequence_closer'),\n",
       " ('U_sequence_closer', '<UNK>'),\n",
       " ('<UNK>', 'U_confirmation'),\n",
       " ('U_confirmation', 'U_sequence_closer'),\n",
       " ('U_sequence_closer', 'A_receipt'),\n",
       " ('A_receipt', 'A_sequence_closer'),\n",
       " ('A_sequence_closer', '<e>')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5642f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add-1 smoothing is performed here. Different value might be better\n",
    "            \n",
    "ngrams_all = {1:[], 2:[], 3:[], 4:[]} #from unigram to fourgram in this case\n",
    "for i in range(4):\n",
    "    for each in unigram:\n",
    "        for j in ngrams(each, i+1):\n",
    "            ngrams_all[i+1].append(j);\n",
    "ngrams_voc = {1:set([]), 2:set([]), 3:set([]), 4:set([])} #set() method is used to convert any of the iterable to sequence of iterable elements with distinct elements\n",
    "\n",
    "for i in range(4):\n",
    "    for gram in ngrams_all[i+1]:\n",
    "        if gram not in ngrams_voc[i+1]:\n",
    "            ngrams_voc[i+1].add(gram)\n",
    "total_ngrams = {1:-1, 2:-1, 3:-1, 4:-1}\n",
    "total_voc = {1:-1, 2:-1, 3:-1, 4:-1}\n",
    "\n",
    "for i in range(4):\n",
    "    total_ngrams[i+1] = len(ngrams_all[i+1])\n",
    "    total_voc[i+1] = len(ngrams_voc[i+1])                       \n",
    "    \n",
    "ngrams_prob = {1:[], 2:[], 3:[], 4:[]}\n",
    "\n",
    "for i in range(4):\n",
    "    for ngram in ngrams_voc[i+1]:\n",
    "        tlist = [ngram]\n",
    "        tlist.append(ngrams_all[i+1].count(ngram))\n",
    "        ngrams_prob[i+1].append(tlist)\n",
    "    \n",
    "for i in range(4):\n",
    "    for ngram in ngrams_prob[i+1]:\n",
    "        ngram[-1] = (ngram[-1])/(total_ngrams[i+1]+total_voc[i+1])             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02b606f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common n-grams without stopword removal and with add-1 smoothing: \n",
      "\n",
      "Most common unigrams:  [[('<UNK>',), 0.3616928670182805], [('U_sequence_closer',), 0.10518423150918414], [('A_greeting',), 0.09755638135227464], [('A_sequence_closer',), 0.08683423187494056], [('U_confirmation',), 0.0864922496214421], [('A_confirmation',), 0.086044198005896], [('U_greeting',), 0.06960710445271867], [('<s>',), 0.03161781380072127], [('<e>',), 0.03161781380072127], [('A_completion_check',), 0.01180295970095755]]\n",
      "\n",
      "Most common bigrams:  [[('<UNK>', '<UNK>'), 0.18398994369754085], [('A_confirmation', 'A_sequence_closer'), 0.05414888536985412], [('U_confirmation', 'U_sequence_closer'), 0.05134980568762422], [('<UNK>', 'U_confirmation'), 0.046080061606177225], [('<UNK>', 'A_confirmation'), 0.04550627858298242], [('U_sequence_closer', '<UNK>'), 0.038437800221585945], [('A_sequence_closer', '<UNK>'), 0.035202720939494204], [('<UNK>', 'A_greeting'), 0.031812871236672285], [('U_greeting', '<UNK>'), 0.02992353963727098], [('A_greeting', '<UNK>'), 0.0285268309623889]]\n",
      "\n",
      "Most common trigrams:  [[('<UNK>', '<UNK>', '<UNK>'), 0.10052403223079957], [('<UNK>', 'A_confirmation', 'A_sequence_closer'), 0.0311990564699619], [('A_confirmation', 'A_sequence_closer', '<UNK>'), 0.03024309111010506], [('<UNK>', 'U_confirmation', 'U_sequence_closer'), 0.02883634533665719], [('<UNK>', '<UNK>', 'U_confirmation'), 0.025498238653254655], [('U_confirmation', 'U_sequence_closer', '<UNK>'), 0.024431521209024157], [('<UNK>', '<UNK>', 'A_confirmation'), 0.0208816254519948], [('U_sequence_closer', '<UNK>', '<UNK>'), 0.016832373805771933], [('A_sequence_closer', '<UNK>', '<UNK>'), 0.016638071903362008], [('A_greeting', '<UNK>', '<UNK>'), 0.014772773640226711]]\n",
      "\n",
      "Most common fourgrams:  [[('<UNK>', '<UNK>', '<UNK>', '<UNK>'), 0.058300660827773736], [('<UNK>', 'A_confirmation', 'A_sequence_closer', '<UNK>'), 0.018945442185474438], [('<UNK>', '<UNK>', 'U_confirmation', 'U_sequence_closer'), 0.01607605843108736], [('A_confirmation', 'A_sequence_closer', '<UNK>', '<UNK>'), 0.015388354886647485], [('<UNK>', '<UNK>', 'A_confirmation', 'A_sequence_closer'), 0.01449117842349891], [('<UNK>', 'U_confirmation', 'U_sequence_closer', '<UNK>'), 0.014052470989976919], [('<UNK>', '<UNK>', '<UNK>', 'U_confirmation'), 0.012781800360451514], [('U_confirmation', 'U_sequence_closer', '<UNK>', '<UNK>'), 0.011412321750403137], [('<UNK>', '<UNK>', '<UNK>', 'A_confirmation'), 0.010046795459575679], [('A_greeting', '<UNK>', '<UNK>', '<UNK>'), 0.007975780187814209]]\n"
     ]
    }
   ],
   "source": [
    "#Prints top 10 unigram, bigram, trigram, fourgram\n",
    "print(\"Most common n-grams without stopword removal and with add-1 smoothing: \\n\")\n",
    "for i in range(4):\n",
    "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "print (\"Most common unigrams: \", str(ngrams_prob[1][:10]))\n",
    "print (\"\\nMost common bigrams: \", str(ngrams_prob[2][:10]))\n",
    "print (\"\\nMost common trigrams: \", str(ngrams_prob[3][:10]))\n",
    "print (\"\\nMost common fourgrams: \", str(ngrams_prob[4][:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc8bdf5",
   "metadata": {},
   "source": [
    "### To calculate probability of unseen sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7d551c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String 1:  {1: ('A_confirmation',), 2: ('A_greeting', 'A_confirmation'), 3: ('<UNK>', 'A_greeting', 'A_confirmation')} \n",
      "String 2:  {1: ('A_confirmation',), 2: ('A_greeting', 'A_confirmation'), 3: ('U_greeting', 'A_greeting', 'A_confirmation')}\n"
     ]
    }
   ],
   "source": [
    "token_1 = ['<s>','U_greeting','<UNK>','A_greeting','A_confirmation']\n",
    "token_2 = ['<s>','U_greeting','A_greeting','A_confirmation']\n",
    "\n",
    "ngram_1 = {1:[], 2:[], 3:[]}                  #to store n-grams formed from the strings\n",
    "ngram_2 = {1:[], 2:[], 3:[]}\n",
    "for i in range(1, 4):\n",
    "    ngram_1[i] = list(ngrams(token_1, i))[-1]\n",
    "    ngram_2[i] = list(ngrams(token_2, i))[-1]\n",
    "\n",
    "print(\"String 1: \", ngram_1,\"\\nString 2: \",ngram_2)\n",
    "\n",
    "for i in range(4):\n",
    "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "pred_1 = {1:[], 2:[], 3:[]}\n",
    "for i in range(3):\n",
    "    count = 0\n",
    "    for each in ngrams_prob[i+2]:\n",
    "        if each[0][:-1] == ngram_1[i+1]:       #to find predictions based on highest probability of n-grams                   \n",
    "            count +=1\n",
    "            pred_1[i+1].append(each[0][-1])\n",
    "            if count ==5:\n",
    "                break\n",
    "    if count<5:\n",
    "        while(count!=5):\n",
    "            pred_1[i+1].append(\"NOT FOUND\")           #if no word prediction is found, replace with NOT FOUND\n",
    "            count +=1\n",
    "            \n",
    "for i in range(4):\n",
    "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "pred_2 = {1:[], 2:[], 3:[]}\n",
    "for i in range(3):\n",
    "    count = 0\n",
    "    for each in ngrams_prob[i+2]:\n",
    "        if each[0][:-1] == ngram_2[i+1]:\n",
    "            count +=1\n",
    "            pred_2[i+1].append(each[0][-1])\n",
    "            if count ==5:\n",
    "                break\n",
    "    if count<5:\n",
    "        while(count!=5):\n",
    "            pred_2[i+1].append(\"\\0\")\n",
    "            count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a0316bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word predictions for the strings using the probability models of bigrams, trigrams, and fourgrams\n",
      "\n",
      "String 1 \n",
      "\n",
      "Bigram model predictions: ['A_sequence_closer', '<UNK>', 'U_confirmation', 'U_sequence_closer', 'A_greeting']\n",
      "Trigram model predictions: ['A_sequence_closer', '<UNK>', 'A_completion_check', 'U_confirmation', 'U_greeting']\n",
      "Fourgram model predictions: ['A_sequence_closer', '<UNK>', 'A_completion_check', 'U_confirmation', 'A_greeting']\n",
      "\n",
      "String 2 \n",
      "\n",
      "Bigram model predictions: ['A_sequence_closer', '<UNK>', 'U_confirmation', 'U_sequence_closer', 'A_greeting']\n",
      "Trigram model predictions: ['A_sequence_closer', '<UNK>', 'A_completion_check', 'U_confirmation', 'U_greeting']\n",
      "Fourgram model predictions: ['A_sequence_closer', '<UNK>', 'U_greeting', 'U_confirmation', 'U_sequence_closer']\n"
     ]
    }
   ],
   "source": [
    "print(\"Next word predictions for the strings using the probability models of bigrams, trigrams, and fourgrams\\n\")\n",
    "print(\"String 1 \\n\")\n",
    "print(\"Bigram model predictions: {}\\nTrigram model predictions: {}\\nFourgram model predictions: {}\\n\" .format(pred_1[1], pred_1[2], pred_1[3]))\n",
    "print(\"String 2 \\n\")\n",
    "print(\"Bigram model predictions: {}\\nTrigram model predictions: {}\\nFourgram model predictions: {}\" .format(pred_2[1], pred_2[2], pred_2[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47964e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dict of probabilities from unigram to fourgram\n",
    "\n",
    "# ngrams_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da998d0",
   "metadata": {},
   "source": [
    "## HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5443a1ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
