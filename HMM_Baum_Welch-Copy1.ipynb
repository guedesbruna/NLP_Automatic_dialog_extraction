{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e148b93",
   "metadata": {},
   "source": [
    "http://www.adeveloperdiary.com/data-science/machine-learning/derivation-and-implementation-of-baum-welch-algorithm-for-hidden-markov-model/\n",
    "\n",
    "Baum-Welch does only find local optima.\n",
    "Insufficient training data can cause parameter estimations to vary wildly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8fc3979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Duplicate key in file './BG.mplstyle', line 21 ('patch.linewidth: 0.5')\n",
      "Duplicate key in file './BG.mplstyle', line 34 ('grid.linestyle: -    # solid line')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pylab inline\n",
    "plt.style.use('./BG.mplstyle')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4549ac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_file = open(\"./src/generated_files/input_hmm.pkl\", \"rb\")\n",
    "input_hmm = pickle.load(a_file)\n",
    "\n",
    "unigram = input_hmm.get('unigram')\n",
    "uni = input_hmm.get('uni')\n",
    "\n",
    "df = pd.read_csv('./data_TM2/processed/processed_utterances_sentence_DA_labeling.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2695d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_as_int(uni, unigram):\n",
    "    '''returns unique list with all observations encoded into numbers.\n",
    "    observation encodings are in the same order as the array weights used as input to the emission prob'''\n",
    "    \n",
    "    unique_DA = list(uni.keys())\n",
    "    da_to_int = {}\n",
    "\n",
    "    for i in unique_DA:\n",
    "        da_to_int[i] = unique_DA.index(i)\n",
    "\n",
    "    obs_as_integ = []\n",
    "\n",
    "    for obs in unigram:\n",
    "        observation_as_int = []\n",
    "        for da in obs:\n",
    "            for k in da_to_int.keys():\n",
    "                if da == k:\n",
    "                    da = da_to_int.get(k)\n",
    "            observation_as_int.append(da) \n",
    "        obs_as_integ.append(observation_as_int)\n",
    "            \n",
    "    return obs_as_integ, unique_DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ba7b58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(V, a, b, initial_distribution): # V=observations, a=trans_p, b=emis_p, initial_dist=pi \n",
    "    α = np.zeros((V.shape[0], a.shape[0])) #shape (n_rows_in_excel * n_hidden_states) Here (500,2) .n_examples (not sure if each is an observation, each is one line of the excel)\n",
    "    α[0, :] = initial_distribution * b[:, V[0]] # apply initial distribution * emission prob on (row 0, and all 2 columns)\n",
    "\n",
    "    for t in range(1, V.shape[0]): # n_examples of csv 500\n",
    "        for j in range(a.shape[0]): # n_hidden_states 2\n",
    "            α[t, j] = α[t - 1].dot(a[:, j]) * b[j, V[t]]\n",
    "            #α[example, state] = α[prev_example] * trans_p for that state * emis_p for that state, for that obs\n",
    " \n",
    "    return α\n",
    " \n",
    "def backward(V, a, b):\n",
    "    β = np.zeros((V.shape[0], a.shape[0]))\n",
    " \n",
    "    # setting β(T) = 1\n",
    "    β[V.shape[0] - 1] = np.ones((a.shape[0]))\n",
    " \n",
    "    # Loop in backward way from T-1 to\n",
    "    # Due to python indexing the actual loop will be T-2 to 0\n",
    "    for t in range(V.shape[0] - 2, -1, -1):\n",
    "        for j in range(a.shape[0]):\n",
    "            β[t, j] = (β[t + 1] * b[:, V[t + 1]]).dot(a[j, :])\n",
    " \n",
    "    return β\n",
    " \n",
    "def baum_welch(V, a, b, initial_distribution, n_iter=100):\n",
    "    M = a.shape[0] # maximization step. shape n_hidden_states >>>> Here 2\n",
    "    T = len(V) # len of observations >>> Here 635219\n",
    " \n",
    "    for n in range(n_iter): #for each epoch\n",
    "        α = forward(V, a, b, initial_distribution)\n",
    "        β = backward(V, a, b)\n",
    " \n",
    "        xi = np.zeros((M, M, T - 1)) #here (2 x 2 x 500)>(n_hid, n_hid, n_observ)\n",
    "        for t in range(T - 1): #for all obs except last (since you need to use future time stamp)\n",
    "            denominator = np.dot(np.dot(α[t, :].T, a) * b[:, V[t + 1]].T, β[t + 1, :])\n",
    "            for i in range(M):\n",
    "                numerator = α[t, i] * a[i, :] * b[:, V[t + 1]].T * β[t + 1, :].T\n",
    "                xi[i, :, t] = numerator / denominator\n",
    " \n",
    "        γ = np.sum(xi, axis=1)\n",
    "        a_n = np.sum(xi, 2) \n",
    "        a_d = np.sum(γ, axis=1).reshape((-1, 1))\n",
    "        a = a_n/a_d\n",
    "        # a = np.sum(xi, 2) / np.sum(γ, axis=1).reshape((-1, 1))\n",
    " \n",
    "        # Add additional T'th element in γ\n",
    "        γ = np.hstack((γ, np.sum(xi[:, :, T - 2], axis=0).reshape((-1, 1))))\n",
    " \n",
    "        K = b.shape[1] #n_of_unique_visible_states. Here 3, in my case n of unique DAs. used to calculate emission of visible state (DA_0 to DA_n) given a hidden state\n",
    "        # denominator = np.sum(γ, axis=1)\n",
    "        b_d = np.sum(γ, axis=1)\n",
    "        for l in range(K):\n",
    "            b[:, l] = np.sum(γ[:, V == l], axis=1) #sum all row values from γ only from column of that DA\n",
    "        b_n = b\n",
    "        # b = np.divide(b, denominator.reshape((-1, 1)))\n",
    "        b = np.divide(b, b_d.reshape((-1, 1)))\n",
    " \n",
    "    return {\"a\":a, \"b\":b, 'a_n':a_n, 'a_d':a_d, 'b_n':b_n, 'b_d':b_d}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e061f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_in_vit_BW_results(a,b,initial_p,unique_DA):\n",
    "    \n",
    "    states = ['New', 'Current']\n",
    "    start_p = {'New':initial_p[0],'Current':initial_p[1]}\n",
    "    trans_p = {'New': {'New':a[0][0],'Current':a[0][1]}, 'Current': {'New': a[1][0],'Current':a[1][1]}} #the higher current, more it appears. 0.6 too litle, 0.7 too much\n",
    "    emis_p = {}\n",
    "    b_new = {}\n",
    "    for da, be in zip(unique_DA, b[0]):\n",
    "        b_new[da] = be\n",
    "        \n",
    "    b_cur = {}\n",
    "    for da, be in zip(unique_DA, b[1]):\n",
    "        b_cur[da] = be\n",
    "    \n",
    "    emis_p['New'] = b_new\n",
    "    emis_p['Current'] = b_cur\n",
    "    \n",
    "    return states, start_p, trans_p, emis_p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9e0f916",
   "metadata": {},
   "outputs": [],
   "source": [
    "#viterbi working version explained from my notebook Cluster_and_HMM_first_approach\n",
    "\n",
    "def viterbi(obs, states, start_p, trans_p, emit_p):\n",
    "    V = [{}]\n",
    "    for st in states:\n",
    "        V[0] [st] = {\"prob\": start_p[st] * emit_p[st] [obs[0]], \"prev\": None} \n",
    "        \n",
    "    # Run Viterbi when t > 0\n",
    "    for t in range(1, len(obs)):\n",
    "        V.append({})\n",
    "        for st in states:\n",
    "            max_tr_prob = V[t - 1] [states[0]] [\"prob\"] * trans_p[states[0]] [st] #0.026402178674778336*0.1 and same*0.9\n",
    "            prev_st_selected = states[0]\n",
    "            \n",
    "            for prev_st in states[1:]:\n",
    "                tr_prob = V[t - 1] [prev_st] [\"prob\"] * trans_p[prev_st] [st] # a diferenca aqui em relacao ao max_tr_prob é que aqui se calcula o previous state\n",
    "                if tr_prob > max_tr_prob: #if other state is higher than state 0 (i.e if Current > New)\n",
    "                    max_tr_prob = tr_prob #then new max is Current instead on New. Here we store the actual probability\n",
    "                    prev_st_selected = prev_st #and previous state is updated. Here we store name of state\n",
    "\n",
    "            max_prob = max_tr_prob * emit_p[st] [obs[t]] #prob of state* emission prob of the DA being observed\n",
    "            V[t] [st] = {\"prob\": max_prob, \"prev\": prev_st_selected}\n",
    "\n",
    "\n",
    "#     for line in dptable(V):\n",
    "#         print(line)\n",
    "\n",
    "    optimal = []\n",
    "    max_prob = 0.0\n",
    "    best_st = None\n",
    "    \n",
    "    # Get most probable state and its backtrack\n",
    "    for st, data in V[-1].items(): #V[-1] is the last dic. ex: {'New': {'prob': 6.983898154776834e-19, 'prev': 'Current'}, 'Current': {'prob': 1.5713770848247866e-18, 'prev': 'New'}\n",
    "        if data[\"prob\"] > max_prob: #6.983898154776834e-19 > 0.0 (in first step)\n",
    "            max_prob = data[\"prob\"] #update max prob to compare with second state in next iteration\n",
    "            best_st = st\n",
    "    optimal.append(best_st) #appending all best states\n",
    "    previous = best_st #updating previous as best state to be used later\n",
    "\n",
    "\n",
    "    # Follow the backtrack till the first observation\n",
    "    for t in range(len(V) - 2, -1, -1): # start=len(V)-2, stop=-1, step=-1. Calculates from last step to first\n",
    "        optimal.insert(0, V[t + 1] [previous] [\"prev\"])\n",
    "        previous = V[t + 1] [previous] [\"prev\"]\n",
    "\n",
    "#     print (\"The steps of states are \" + \" \".join(optimal) + \" with highest probability of %s\" % max_prob)\n",
    "    return optimal, max_prob\n",
    "\n",
    "def dptable(V):\n",
    "    # Print a table of steps from dictionary\n",
    "    yield \" \" * 5 + \"     \".join((\"%3d\" % i) for i in range(len(V)))\n",
    "    for state in V[0]:\n",
    "        yield \"%.7s: \" % state + \" \".join(\"%.7s\" % (\"%lf\" % v[state] [\"prob\"]) for v in V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56c0d55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_dialogues_hmm(unigram, uni):\n",
    "    \n",
    "    obs_as_integ, unique_DA = obs_as_int(uni, unigram)\n",
    "    hmm_seq = []\n",
    "    failed_indexes = []\n",
    "\n",
    "    a_nominator = []\n",
    "    a_denominator = []\n",
    "    b_nominator = []\n",
    "    b_denominator = []\n",
    "    bw_ind = []\n",
    "\n",
    "    for inte in obs_as_integ:\n",
    "\n",
    "        V = np.array(inte) #testing with 2 observations\n",
    "\n",
    "        # Transition Probabilities >>>>>> shape (N_hidden_states * N_hidden_states)\n",
    "        # a = np.array(((0.1, 0.9), (0.4,0.6))) #if I want to input my own perception as initial state. each row (hidden state) sum to 1\n",
    "        a = np.ones((2, 2))\n",
    "        a = a / np.sum(a, axis=1)\n",
    "        \n",
    "        # Emission Probabilities >>>>>> # shape (rows = N_hidden_states, columns = N_DAs). (prob of given a state observing a specific DA)\n",
    "        b = np.array((list(uni.values()), list(uni.values()))) #using unigram probabilities. each row (hidden state) sum to 1  \n",
    "\n",
    "        # Probabilities for the initial distribution\n",
    "        initial_distribution = np.array((0.9999, 0.0001))\n",
    "\n",
    "        bw = baum_welch(V, a, b, initial_distribution, n_iter=100)\n",
    "\n",
    "        a_nominator.append(bw.get('a_n'))\n",
    "        a_denominator.append(bw.get('a_d'))\n",
    "        b_nominator.append(bw.get('b_n'))\n",
    "        b_denominator.append(bw.get('b_d'))\n",
    "        bw_ind.append(bw)\n",
    "\n",
    "    a_total = np.sum(np.array(a_nominator),axis=0)/np.sum(np.array(a_denominator),axis=0)\n",
    "    b_total = sum([np.divide(b_nom, b_den.reshape((-1, 1))) for b_nom,b_den in zip(b_nominator,b_denominator)],axis=0)\n",
    "    \n",
    "    states, start_p, trans_p, emis_p = input_in_vit_BW_results(a_total,b_total, initial_distribution, unique_DA)\n",
    "    print(trans_p)\n",
    "    print(emis_p)\n",
    "    \n",
    "    for e, dialog in enumerate(unigram):\n",
    "        try:\n",
    "            seq, p = viterbi(dialog, states, start_p, trans_p, emis_p)\n",
    "            hmm_seq.append([dialog[2:-2], seq[2:-2], p]) #remove extra symbols from dialog (<s>, <ss>, <e>, <ee>)\n",
    "        except Exception:\n",
    "            failed_indexes.append(e)\n",
    "            pass\n",
    "\n",
    "    return hmm_seq, failed_indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14bb454f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'New': {'New': 0.44409616068135843, 'Current': 0.5559038393186927}, 'Current': {'New': 0.4428616502521786, 'Current': 0.5571383497477682}}\n",
      "{'New': {'<UNK>': 2198.8866181253097, 'A_detail_request': 1571.693396544867, 'U_answer': 1417.2025253843372, 'A_greeting': 1427.773141486941, 'U_confirmation': 1182.0196421775247, 'A_confirmation': 1101.8848079940667, 'U_partial_request': 1303.2570986779228, 'A_grant': 1192.8241407338594, 'U_greeting': 1061.9844709207366, 'U_sequence_closer': 1007.2938491968105, 'A_sequence_closer': 592.9778446925834, '<ee>': 507.344157211922, '<e>': 507.344157211922, '<ss>': 507.344157211922, '<s>': 1187.6614673891024, 'A_completion_check': 148.33895897543658, 'A_hold_request': 150.49333620073892, 'A_receipt': 119.06952074969234, 'A_repair_initiator': 36.69684832252892, 'A_disconfirmation': 29.122439651056297, 'U_disconfirmation': 16.44072509413651, 'U_repair_initiator': 12.338143412672412, 'U_receipt': 4.039832880017887, 'U_completion_check': 2.846339152443732, 'U_hold_request': 2.1223806013381283}, 'Current': {'<UNK>': 2353.6306194957165, 'A_detail_request': 1679.9743924597108, 'U_answer': 1514.3448670970547, 'A_greeting': 1534.818044438069, 'U_confirmation': 1264.9235793206178, 'A_confirmation': 1174.387343230032, 'U_partial_request': 1405.513732905442, 'A_grant': 1287.3214091385314, 'U_greeting': 1145.358811276886, 'U_sequence_closer': 1084.1065400394575, 'A_sequence_closer': 637.3901122095747, '<ee>': 550.0650356655827, '<e>': 550.0650356655827, '<ss>': 550.0650356655827, '<s>': 8.471196395222306e-18, 'A_completion_check': 157.62803668021044, 'A_hold_request': 161.0877522355574, 'A_receipt': 128.1628967314975, 'A_repair_initiator': 39.08776250010249, 'A_disconfirmation': 30.936452208891644, 'U_disconfirmation': 17.47713845679351, 'U_repair_initiator': 13.079352560915595, 'U_receipt': 4.315491710092318, 'U_completion_check': 2.994933458113107, 'U_hold_request': 2.265624849495907}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3s/9v17gzrj1kvctdbg2vd9d2qw0000gn/T/ipykernel_14776/4044480997.py:21: RuntimeWarning: overflow encountered in double_scalars\n",
      "  max_prob = max_tr_prob * emit_p[st] [obs[t]] #prob of state* emission prob of the DA being observed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of dialogues that failed and went to exception: 0\n"
     ]
    }
   ],
   "source": [
    "model_name = 'HMM'\n",
    "\n",
    "unique_ids = input_hmm.get('unique_ids') # same as input_hmm['unique_ids']\n",
    "\n",
    "hmm_seq, failed_indexes = all_dialogues_hmm(unigram, uni)\n",
    "print('Count of dialogues that failed and went to exception: ' + str(len(failed_indexes)))\n",
    "\n",
    "### Manual Inspection\n",
    "# i = 0\n",
    "\n",
    "# #sanity check first\n",
    "# print(len(hmm_seq[i][0]) == len(hmm_seq[i][1]) == len(df['new_text'].loc[df['conversation_id'] == unique_ids[i]]))\n",
    "# print(len(hmm_seq[i][0]))\n",
    "# print(len(hmm_seq[i][1]))\n",
    "# print(len(df['new_text'].loc[df['conversation_id'] == unique_ids[i]]))\n",
    "\n",
    "# ################## THE TEST INSPECTION IS NOT MATCHING RN. TAKING TEXT FROM OTHER DIALOGUES \n",
    "\n",
    "# teste_inspection = pd.DataFrame(list(zip(hmm_seq[i][0], hmm_seq[i][1], df['new_text'].loc[df['conversation_id'] == unique_ids[i]])),columns =['DA', 'HMM', 'Text'])\n",
    "# teste_inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d1d040",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
