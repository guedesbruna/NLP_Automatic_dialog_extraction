{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e148b93",
   "metadata": {},
   "source": [
    "http://www.adeveloperdiary.com/data-science/machine-learning/derivation-and-implementation-of-baum-welch-algorithm-for-hidden-markov-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8fc3979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Duplicate key in file './BG.mplstyle', line 21 ('patch.linewidth: 0.5')\n",
      "Duplicate key in file './BG.mplstyle', line 34 ('grid.linestyle: -    # solid line')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "plt.style.use('./BG.mplstyle')\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62456bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# def forward(V, a, b, initial_distribution): # V=observations, a=trans_p, b=emis_p, initial_dist=pi \n",
    "#     alpha = np.zeros((V.shape[0], a.shape[0])) #shape (n_rows_in_excel * n_hidden_states) Here (500,2) .n_examples (not sure if each is an observation, each is one line of the excel)\n",
    "#     alpha[0, :] = initial_distribution * b[:, V[0]] # apply initial distribution * emission prob on (row zero, and all 2 columns)\n",
    "#     #print(alpha[0, :]) #results in for example [0.05555556 0.08333333]\n",
    "#     for t in range(1, V.shape[0]): # n_examples of csv 500\n",
    "#         for j in range(a.shape[0]): # n_hidden_states 2\n",
    "#             # Matrix Computation Steps\n",
    "#             # ((1x2) . (1x2)) * (1)\n",
    "#             # (1)             * (1)\n",
    "#             alpha[t, j] = alpha[t - 1].dot(a[:, j]) * b[j, V[t]]\n",
    "#             #alpha[example, state] = alpha[prev_example] * trans_p for that state * emis_p for that state, for that obs\n",
    " \n",
    "#     return alpha\n",
    " \n",
    "# def backward(V, a, b):\n",
    "#     beta = np.zeros((V.shape[0], a.shape[0]))\n",
    " \n",
    "#     # setting beta(T) = 1\n",
    "#     beta[V.shape[0] - 1] = np.ones((a.shape[0]))\n",
    " \n",
    "#     # Loop in backward way from T-1 to\n",
    "#     # Due to python indexing the actual loop will be T-2 to 0\n",
    "#     for t in range(V.shape[0] - 2, -1, -1):\n",
    "#         for j in range(a.shape[0]):\n",
    "#             beta[t, j] = (beta[t + 1] * b[:, V[t + 1]]).dot(a[j, :])\n",
    " \n",
    "#     return beta\n",
    " \n",
    "# def baum_welch(V, a, b, initial_distribution, n_iter=100):\n",
    "#     M = a.shape[0] # maximization step. shape n_hidden_states. Here 2\n",
    "#     T = len(V) # len of observations\n",
    " \n",
    "#     for n in range(n_iter): #for each epoch\n",
    "#         alpha = forward(V, a, b, initial_distribution)\n",
    "#         beta = backward(V, a, b)\n",
    " \n",
    "#         xi = np.zeros((M, M, T - 1)) #here (2 x 2 x 500)>(n_hid, n_hid, n_observ)\n",
    "#         for t in range(T - 1): #for all obs except last (since you need to use future time stamp)\n",
    "#             denominator = np.dot(np.dot(alpha[t, :].T, a) * b[:, V[t + 1]].T, beta[t + 1, :])\n",
    "#             for i in range(M):\n",
    "#                 numerator = alpha[t, i] * a[i, :] * b[:, V[t + 1]].T * beta[t + 1, :].T\n",
    "#                 xi[i, :, t] = numerator / denominator\n",
    " \n",
    "#         gamma = np.sum(xi, axis=1)\n",
    "#         a = np.sum(xi, 2) / np.sum(gamma, axis=1).reshape((-1, 1))\n",
    " \n",
    "#         # Add additional T'th element in gamma\n",
    "#         gamma = np.hstack((gamma, np.sum(xi[:, :, T - 2], axis=0).reshape((-1, 1))))\n",
    " \n",
    "#         K = b.shape[1] #n_of_unique_visible_states. Here 3, in my case n of unique DAs. used to calculate emission of visible state (DA_0 to DA_n) given a hidden state\n",
    "#         denominator = np.sum(gamma, axis=1)\n",
    "#         for l in range(K):\n",
    "#             b[:, l] = np.sum(gamma[:, V == l], axis=1) #sum all row values from gamma only from column of that DA\n",
    " \n",
    "#         b = np.divide(b, denominator.reshape((-1, 1)))\n",
    " \n",
    "#     return {\"a\":a, \"b\":b}\n",
    " \n",
    "    \n",
    "# data = pd.read_csv('data_python.csv')\n",
    " \n",
    "# V = data['Visible'].values # each row of csv corresponding to observations\n",
    " \n",
    "# # Transition Probabilities >>>>>> shape (N_hidden_states * N_hidden_states)\n",
    "# a = np.ones((2, 2))\n",
    "# a = a / np.sum(a, axis=1)\n",
    " \n",
    "# # Emission Probabilities >>>>>> # shape (rows = N_hidden_states, columns = N_DAs). (prob of given a state observing a specific DA)\n",
    "# b = np.array(((1, 3, 5), (2, 4, 6)))\n",
    "# print(b)\n",
    "# b = b / np.sum(b, axis=1).reshape((-1, 1))\n",
    "# print(b)\n",
    " \n",
    "# # Probabilities for the initial distribution\n",
    "# initial_distribution = np.array((0.5, 0.5))\n",
    " \n",
    "# print(baum_welch(V, a, b, initial_distribution, n_iter=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5b4892",
   "metadata": {},
   "source": [
    "## applied to my case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4549ac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "a_file = open(\"./src/generated_files/input_hmm.pkl\", \"rb\")\n",
    "input_hmm = pickle.load(a_file)\n",
    "\n",
    "unigram = input_hmm.get('unigram')\n",
    "bi_fut = input_hmm.get('bi_fut')\n",
    "bi_past = input_hmm.get('bi_past')\n",
    "uni = input_hmm.get('uni')\n",
    "\n",
    "df = pd.read_csv('./data_TM2/processed/processed_utterances_sentence_DA_labeling.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a0022c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_as_int(uni, unigram):\n",
    "    '''returns unique list with all observations encoded into numbers.\n",
    "    observation encodings are in the same order as the array weights used as input to the emission prob'''\n",
    "    \n",
    "    unique_DA = list(uni.keys())\n",
    "    da_to_int = {}\n",
    "\n",
    "    for i in unique_DA:\n",
    "        da_to_int[i] = unique_DA.index(i)\n",
    "\n",
    "    observation_as_int = []\n",
    "\n",
    "    for obs in unigram:\n",
    "        for da in obs:\n",
    "            for k in da_to_int.keys():\n",
    "                if da == k:\n",
    "                    da = da_to_int.get(k)\n",
    "            observation_as_int.append(da)  \n",
    "    return observation_as_int, unique_DA\n",
    "\n",
    "observation_as_int, unique_DA = obs_as_int(uni, unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3b8f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "def forward(V, a, b, initial_distribution): # V=observations, a=trans_p, b=emis_p, initial_dist=pi \n",
    "    alpha = np.zeros((V.shape[0], a.shape[0])) #shape (n_rows_in_excel * n_hidden_states) Here (500,2) .n_examples (not sure if each is an observation, each is one line of the excel)\n",
    "    alpha[0, :] = initial_distribution * b[:, V[0]] # apply initial distribution * emission prob on (row 0, and all 2 columns)\n",
    "    #print(alpha[0, :]) #results in for example [0.05555556 0.08333333]\n",
    "    for t in range(1, V.shape[0]): # n_examples of csv 500\n",
    "        for j in range(a.shape[0]): # n_hidden_states 2\n",
    "            # Matrix Computation Steps\n",
    "            # ((1x2) . (1x2)) * (1)\n",
    "            # (1)             * (1)\n",
    "            alpha[t, j] = alpha[t - 1].dot(a[:, j]) * b[j, V[t]]\n",
    "            #alpha[example, state] = alpha[prev_example] * trans_p for that state * emis_p for that state, for that obs\n",
    " \n",
    "    return alpha\n",
    " \n",
    "def backward(V, a, b):\n",
    "    beta = np.zeros((V.shape[0], a.shape[0]))\n",
    " \n",
    "    # setting beta(T) = 1\n",
    "    beta[V.shape[0] - 1] = np.ones((a.shape[0]))\n",
    " \n",
    "    # Loop in backward way from T-1 to\n",
    "    # Due to python indexing the actual loop will be T-2 to 0\n",
    "    for t in range(V.shape[0] - 2, -1, -1):\n",
    "        for j in range(a.shape[0]):\n",
    "            beta[t, j] = (beta[t + 1] * b[:, V[t + 1]]).dot(a[j, :])\n",
    " \n",
    "    return beta\n",
    " \n",
    "def baum_welch(V, a, b, initial_distribution, n_iter=100):\n",
    "    M = a.shape[0] # maximization step. shape n_hidden_states >>>> Here 2\n",
    "    T = len(V) # len of observations >>> Here 635219\n",
    " \n",
    "    for n in range(n_iter): #for each epoch\n",
    "        alpha = forward(V, a, b, initial_distribution)\n",
    "        beta = backward(V, a, b)\n",
    " \n",
    "        xi = np.zeros((M, M, T - 1)) #here (2 x 2 x 500)>(n_hid, n_hid, n_observ)\n",
    "        for t in range(T - 1): #for all obs except last (since you need to use future time stamp)\n",
    "            denominator = np.dot(np.dot(alpha[t, :].T, a) * b[:, V[t + 1]].T, beta[t + 1, :])\n",
    "            for i in range(M):\n",
    "                numerator = alpha[t, i] * a[i, :] * b[:, V[t + 1]].T * beta[t + 1, :].T\n",
    "                xi[i, :, t] = numerator / denominator\n",
    " \n",
    "        gamma = np.sum(xi, axis=1)\n",
    "        a = np.sum(xi, 2) / np.sum(gamma, axis=1).reshape((-1, 1))\n",
    " \n",
    "        # Add additional T'th element in gamma\n",
    "        gamma = np.hstack((gamma, np.sum(xi[:, :, T - 2], axis=0).reshape((-1, 1))))\n",
    " \n",
    "        K = b.shape[1] #n_of_unique_visible_states. Here 3, in my case n of unique DAs. used to calculate emission of visible state (DA_0 to DA_n) given a hidden state\n",
    "        denominator = np.sum(gamma, axis=1)\n",
    "        for l in range(K):\n",
    "            b[:, l] = np.sum(gamma[:, V == l], axis=1) #sum all row values from gamma only from column of that DA\n",
    " \n",
    "        b = np.divide(b, denominator.reshape((-1, 1)))\n",
    " \n",
    "    return {\"a\":a, \"b\":b}\n",
    " \n",
    "V = np.array(observation_as_int)# shape (635219,) # each row of csv corresponding to observations\n",
    "# V = np.array(observation_as_int[0:49]) #one example only\n",
    "# V = np.array(observation_as_int[0:100]) #testing with 2 observations\n",
    " \n",
    "# Transition Probabilities >>>>>> shape (N_hidden_states * N_hidden_states)\n",
    "a = np.array(((0.1, 0.9), (0.6,0.4))) #if I want to input my own perception as initial state. each row (hidden state) sum to 1\n",
    " \n",
    "# Emission Probabilities >>>>>> # shape (rows = N_hidden_states, columns = N_DAs). (prob of given a state observing a specific DA)\n",
    "# b = np.array((list(uni.values()), list(uni.values()))) #using unigram probabilities. each row (hidden state) sum to 1  \n",
    "b = np.ones((2, len(list(uni.values())))) #starts with ones\n",
    "b = b / np.sum(b, axis=1).reshape((-1, 1)) #normalized per row\n",
    "    \n",
    "# Probabilities for the initial distribution\n",
    "initial_distribution = np.array((0.9999, 0.0001))\n",
    " \n",
    "bw = baum_welch(V, a, b, initial_distribution, n_iter=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c935a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_in_vit_BW_results(bw, initial_p):\n",
    "    \n",
    "    states = ['New', 'Current']\n",
    "    start_p = {'New':initial_p[0],'Current':initial_p[1]}\n",
    "    trans_p = {'New': {'New':bw.get('a')[0][0],'Current':bw.get('a')[0][1]}, 'Current': {'New': bw.get('a')[1][0],'Current':bw.get('a')[1][1]}} #the higher current, more it appears. 0.6 too litle, 0.7 too much\n",
    "    emis_p = {}\n",
    "    b_new = {}\n",
    "    for da, be in zip(unique_DA, bw.get('b')[0]):\n",
    "        b_new[da] = be\n",
    "        \n",
    "    b_cur = {}\n",
    "    for da, be in zip(unique_DA, bw.get('b')[1]):\n",
    "        b_cur[da] = be\n",
    "    \n",
    "    emis_p['New'] = b_new\n",
    "    emis_p['Current'] = b_cur\n",
    "    \n",
    "    return states, start_p, trans_p, emis_p\n",
    "\n",
    "states, start_p, trans_p, emis_p = input_in_vit_BW_results(bw, initial_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c42ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(obs, states, start_p, trans_p, emis_p): \n",
    "    ''' #code based on : https://en.wikipedia.org/wiki/Viterbi_algorithm#Pseudocode\n",
    "    Code should receive emition prob of uni, bi and trigrams\n",
    "    This also requires as input sentences (i.e: obs) with padding=2 on each side'''\n",
    "\n",
    "    V = [{}]\n",
    "\n",
    "    for st in states:\n",
    "        V[0] [st] = {\"prob\": start_p[st] * emis_p[st] [obs[0]], \"prev\": None} #### OLD\n",
    "        V.append({})\n",
    "        V[1] [st] = {\"prob\": start_p[st] * emis_p[st] [obs[1]], \"prev\": None} #### NEW\n",
    "\n",
    "\n",
    "    # Run Viterbi when t > 1\n",
    "    for t in range(2, len(obs)-2): \n",
    "        V.append({})\n",
    "        for st in states: #New and Current            \n",
    "            max_tr_prob = V[t - 1] [states[0]] [\"prob\"] * trans_p[states[0]] [st] #0.026402178674778336*0.1 and same*0.9 ######### NEW\n",
    "            prev_st_selected = states[0] \n",
    "            \n",
    "            for prev_st in states[1:]: #(states[1:]) >>> Current\n",
    "                tr_prob = V[t - 1] [prev_st] [\"prob\"] * trans_p[prev_st] [st] # a diferenca aqui em relacao ao max_tr_prob é que aqui se calcula o previous state\n",
    "                if tr_prob > max_tr_prob: #if other state is higher than state 0 (i.e if Current > New)\n",
    "                    max_tr_prob = tr_prob #then new max is Current instead on New. Here we store the actual probability\n",
    "                    prev_st_selected = prev_st #and previous state is updated. Here we store name of state\n",
    "\n",
    "            max_prob = max_tr_prob * emis_p[st] [obs[t]] \n",
    "            \n",
    "            V[t] [st] = {\"prob\": max_prob, \"prev\": prev_st_selected}\n",
    "\n",
    "    V = V[:-1]\n",
    "    # for line in dptable(V):\n",
    "       # print(line)\n",
    "\n",
    "    optimal = []\n",
    "    max_prob = 0.0\n",
    "    best_st = None\n",
    "    \n",
    "    # Get most probable state and its backtrack\n",
    "    for st, data in V[-1].items(): # V[-1] is the last dic. ex: {'New': {'prob': 6.983898154776834e-19, 'prev': 'Current'}, 'Current': {'prob': 1.5713770848247866e-18, 'prev': 'New'}\n",
    "        if data[\"prob\"] > max_prob: #6.983898154776834e-19 > 0.0 (in first step)\n",
    "            max_prob = data[\"prob\"] #update max prob to compare with second state in next iteration\n",
    "            best_st = st\n",
    "    optimal.append(best_st) #appending all best states\n",
    "    previous = best_st #updating previous as best state to be used later\n",
    "\n",
    "\n",
    "    # Follow the backtrack till the first observation\n",
    "    for t in range(len(V) - 2, -1, -1): # start=len(V)-2, stop=-1, step=-1. Calculates from last step to first ###### NEW\n",
    "        optimal.insert(0, V[t + 1] [previous] [\"prev\"])\n",
    "        previous = V[t + 1] [previous] [\"prev\"]\n",
    "    optimal = optimal[1:]\n",
    "    \n",
    "    # print (\"The steps of states are \" + \" \".join(optimal) + \" with highest probability of %s\" % max_prob)\n",
    "    return optimal, max_prob\n",
    "\n",
    "\n",
    "\n",
    "def dptable(V):\n",
    "    # Print a table of steps from dictionary\n",
    "    yield \" \" * 5 + \"     \".join((\"%3d\" % i) for i in range(len(V)))\n",
    "    for state in V[0]:\n",
    "        yield \"%.7s: \" % state + \" \".join(\"%.7s\" % (\"%lf\" % v[state] [\"prob\"]) for v in V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f3b71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_dialog_hmm(unigram, bw, initial_distribution):\n",
    "    states, start_p, trans_p, emis_p = input_in_vit_BW_results(bw, initial_distribution)\n",
    "\n",
    "    hmm_seq = []\n",
    "    count = 0\n",
    "    for dialog in unigram:\n",
    "        try:\n",
    "            seq, p = viterbi(dialog, states, start_p, trans_p, emis_p)\n",
    "            # hmm_seq.append([dialog[2:-1], seq, p]) #remove extra symbols from dialog (<s>, <ss>, <e>)\n",
    "            hmm_seq.append([dialog[2:-2], seq[:-1], p]) #remove extra symbols from dialog (<s>, <ss>, <e>, <ee>)\n",
    "        except Exception:\n",
    "            count+=1\n",
    "            pass\n",
    "\n",
    "    return hmm_seq, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d894ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'HMM'\n",
    "\n",
    "unique_ids = input_hmm.get('unique_ids') # same as input_hmm['unique_ids']\n",
    "\n",
    "hmm_seq, count = all_dialog_hmm(unigram, bw, initial_distribution)\n",
    "print('Count of dialogues that failed and went to exception: ' + str(count))\n",
    "\n",
    "# i = 10\n",
    "# inspection_df = manual_inspection(i, df, hmm_seq, unique_ids)\n",
    "\n",
    "# sorted_d, seq_sizes, seq_to_plot = sorted_seq_and_counts_hmm(hmm_seq)\n",
    "# plot_literal_count_per_seq(seq_sizes, model_name)\n",
    "# plot_len_grouped = plot_grouped_per_len_seq(sorted_d, seq_to_plot, model_name)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b53d15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
