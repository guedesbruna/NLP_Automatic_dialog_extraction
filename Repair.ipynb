{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e943a33b",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "\n",
    "## All preprocessing just needs to be done once. \n",
    "### If already done go direclty to DA patterns to load csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de9d64ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.io.json import json_normalize\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ccba6b",
   "metadata": {},
   "source": [
    "#### Open all datasets from different tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e7d0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t1 = pd.read_json('./data_TM2/flights.json')\n",
    "df_t2 = pd.read_json('./data_TM2/food-ordering.json')\n",
    "df_t3 = pd.read_json('./data_TM2/hotels.json')\n",
    "df_t4 = pd.read_json('./data_TM2/movies.json')\n",
    "df_t5 = pd.read_json('./data_TM2/music.json')\n",
    "df_t6 = pd.read_json('./data_TM2/restaurant-search.json')\n",
    "df_t7 = pd.read_json('./data_TM2/sports.json')\n",
    "\n",
    "# create extra column in the beginning for type of task before merging\n",
    "df_t1.insert(0, 'task', 'flights')\n",
    "df_t2.insert(0, 'task', 'food-ordering')\n",
    "df_t3.insert(0, 'task', 'hotels')\n",
    "df_t4.insert(0, 'task', 'movies')\n",
    "df_t5.insert(0, 'task', 'music')\n",
    "df_t6.insert(0, 'task', 'restaurant-search')\n",
    "df_t7.insert(0, 'task', 'sports')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7cfaa8",
   "metadata": {},
   "source": [
    "#### Normalize json and change to pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867f1f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_raw(df): #, name):\n",
    "    dialogs = []\n",
    "\n",
    "    for e,i in enumerate(df['utterances']):\n",
    "        for j in i:\n",
    "            new_df=pd.json_normalize(j)\n",
    "            new_df.insert(0, 'task', df['task'][e])\n",
    "            new_df.insert(1, 'conversation_id', df['conversation_id'][e])\n",
    "            new_df.insert(2, 'instruction_id', df['instruction_id'][e])\n",
    "        \n",
    "            dialogs.append(new_df)\n",
    "\n",
    "    large_df = pd.concat(dialogs, ignore_index=True)\n",
    "#   large_df.to_csv('./data_TM2/processed_utterances_'+name+'.csv')\n",
    "    \n",
    "    return large_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43582db7",
   "metadata": {},
   "source": [
    "#### Create unique dataset with normalized tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607586e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(df_list):\n",
    "    large_frames = []\n",
    "\n",
    "    for df_ in df_list:\n",
    "        large_df = process_raw(df_)\n",
    "        large_frames.append(large_df)\n",
    "\n",
    "    all_tasks = pd.concat(large_frames, ignore_index=True)\n",
    "    all_tasks.to_csv('./data_TM2/concatenated_tasks.csv')\n",
    "    \n",
    "    return all_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03061b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [df_t1, df_t2, df_t3, df_t4, df_t5, df_t6, df_t7]\n",
    "all_tasks = unique(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb3bd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #essa parte é de antes porem com split incluindo simbolo do split\n",
    "# new_text = []\n",
    "# for ut in df['text']:\n",
    "#     new_ut = re.split('(\\.)', ut)\n",
    "#     new_text.append(new_ut)\n",
    "# df['new_text'] = new_text\n",
    "\n",
    "# #to keep delimiter in sentence\n",
    "# #not very efficient way of doing it, apparently a better way using regex symbols\n",
    "\n",
    "# new_lista = []\n",
    "# for row in df['new_text']:\n",
    "# #     print(row)\n",
    "#     for e, word in enumerate(row):\n",
    "#         if word == '.':\n",
    "#             new_lista.append(row[e-1]+row[e])\n",
    "#             new_lista.remove(row[e-1])\n",
    "#         else:\n",
    "#             new_lista.append(word)\n",
    "\n",
    "# df = df.explode('new_text')\n",
    "# df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3bf22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "df = pd.read_csv('./data_TM2/concatenated_tasks.csv', index_col=0)\n",
    "\n",
    "tokenized = []\n",
    "for ut in df['text']:\n",
    "    tokenized.append(sent_tokenize(ut))\n",
    "    \n",
    "df['new_lista'] = tokenized\n",
    "df = df.explode('new_lista')\n",
    "df = df.reset_index(drop=True)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9b2f80",
   "metadata": {},
   "source": [
    "## Load gold label dataset to work with DA patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "453fdeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#format golden label to compare\n",
    "#then process automatic labels using this file\n",
    "\n",
    "df = pd.read_csv('./data_TM2/sample_b1_goldannotated_noAutomaticLabels.csv', index_col=0)\n",
    "df = df.iloc[:2034]\n",
    "\n",
    "gold = []\n",
    "for label in df['gold_labels']:\n",
    "    gold.append(str(label).split(','))\n",
    "\n",
    "df['gold_formatted'] = gold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478c38d6",
   "metadata": {},
   "source": [
    "## Load whole dataset to work with DA patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c635da6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('./data_TM2/processed_utterances_all_tasks.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b7d4de",
   "metadata": {},
   "source": [
    "### Find substrings\n",
    "make list of words and rules for labeling DA\n",
    "https://towardsdatascience.com/check-for-a-substring-in-a-pandas-dataframe-column-4b949f64852#:~:text=Using%20%E2%80%9Ccontains%E2%80%9D%20to%20Find%20a,substring%20and%20False%20if%20not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4920aabb",
   "metadata": {},
   "source": [
    "- add new column for subset. since each utterance can only be one thing one column for all DA should be enough. \n",
    "- Later I'll have to check the vocabulary used for generic rules (FS_base_greeting or whatever is)\n",
    "- Also study in the UX book what is repair and other patterns:\n",
    "    - page 243 to 246: NCF pattern language summary\n",
    "\n",
    "Tips:\n",
    "- #NOT_INCLUDE_WORD: \n",
    "    - not_fifa = football_soccer_games.loc[~football_soccer_games['Name'].str.contains('FIFA')]\n",
    "- #to make a new dataframe with slicing subset:\n",
    "    - new_df = df.loc[df['text'].str.contains('|'.join(end_of_request), case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51656f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DA indicators. Obs: the vars bellow are sensitive to spacing !!! 'hi' != ' hi'\n",
    "\n",
    "greeting = [' hi','hi.','hello','bye'] \n",
    "repair_initiator = ['sorry','repeat', 'understand', 'mean'] \n",
    "request_summary = ['correct?','confirm'] #when you ask an information to be confirmed\n",
    "confirmation = ['yes','correct.','correct ', 'that\\'s it', 'indeed'] #confirmation é a resposta que confirma: ex yes | correct | that's it\n",
    "sentence_closer = ['awesome','great','perfect','exactly','that\\'s all','that is all', 'thanks', 'thank you', 'pleasure', 'excellent'] \n",
    "question = ['\\?']\n",
    "receipt = ['You are welcome', 'you\\'re welcome']\n",
    "grant = ['got it', 'sure']\n",
    "\n",
    "## !!!!! order-sensitive > hierarchical!!!! this variable chooses only one label, rules goes on order. once it assigns one rule it won't check next\n",
    "\n",
    "temp2=df.new_text.fillna(\"0\")\n",
    "df['DA_indicators_sent'] =  np.where(temp2.str.contains('|'.join(repair_initiator), case=False), \"repair_initiator\",\n",
    "                            np.where(temp2.str.contains('|'.join(greeting), case=False), \"greeting\",\n",
    "                            np.where(temp2.str.contains('|'.join(request_summary), case=False),  \"request_summary\",\n",
    "                            np.where(temp2.str.contains('|'.join(confirmation), case=False),  \"confirmation\",\n",
    "                            np.where(temp2.str.contains('|'.join(receipt), case=False),  \"receipt\",\n",
    "                            np.where(temp2.str.contains('|'.join(grant), case=False),  \"grant\",\n",
    "                            np.where(temp2.str.contains('|'.join(question), case=False),  \"question\",\n",
    "                            np.where(temp2.str.contains('|'.join(sentence_closer), case=False), \"sentence_closer\", \"x\"))))))))\n",
    "\n",
    "df['DA_indicators_sent'].value_counts()\n",
    "\n",
    "#it works! #binary variables and in the end gather in one new column\n",
    "\n",
    "df['DA_rep'] =  np.where(temp2.str.contains('|'.join(repair_initiator), case=False), \"repair_initiator\", '')\n",
    "df['DA_greet'] =  np.where(temp2.str.contains('|'.join(greeting), case=False), \"greeting\", '')\n",
    "df['DA_req_sum'] =  np.where(temp2.str.contains('|'.join(request_summary), case=False), \"request_summary\", '')\n",
    "df['DA_conf'] =  np.where(temp2.str.contains('|'.join(confirmation), case=False), \"confirmation\", '')\n",
    "df['DA_receipt'] =  np.where(temp2.str.contains('|'.join(receipt), case=False), \"receipt\", '')\n",
    "df['DA_grant'] =  np.where(temp2.str.contains('|'.join(grant), case=False), \"grant\", '')\n",
    "df['DA_quest'] =  np.where(temp2.str.contains('|'.join(question), case=False), \"question\", '')\n",
    "df['DA_closer'] =  np.where(temp2.str.contains('|'.join(sentence_closer), case=False), \"sentence_closer\", '')\n",
    "df['all_DA'] = df[['DA_rep', 'DA_greet','DA_req_sum', 'DA_conf', 'DA_receipt', 'DA_grant', 'DA_quest', 'DA_closer']].agg(' '.join, axis=1)\n",
    "\n",
    "df['all_DA'] = [word_tokenize(da) for da in df['all_DA']]\n",
    "\n",
    "# df.loc[:, ['speaker', 'new_text', 'all_DA']].to_csv('analysis_multiple_DA_cases.csv')\n",
    "df.loc[:, ['speaker', 'new_text', 'all_DA']]\n",
    "counts = df['all_DA'].value_counts()\n",
    "\n",
    "# counts.to_csv('count_overlaping_DAs.csv')\n",
    "data = {'counts':counts}\n",
    "df_counts = pd.DataFrame(data)\n",
    "df_counts.head(45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f163b77",
   "metadata": {},
   "source": [
    "## Second iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65652e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DA indicators. Obs: the vars bellow are sensitive to spacing !!! 'hi' != ' hi'\n",
    "\n",
    "greeting = [' hi','hi.','hello','yo ', 'hey'] \n",
    "repair_initiator = ['sorry','repeat', 'understand', 'mean ', 'what?', 'example?']\n",
    "repair = ['I meant']\n",
    "request_summary = [' correct?','confirm']  \n",
    "confirmation = ['yes',' correct.',' correct ', 'that\\'s it', 'indeed', 'yeah', 'that\\'s right', 'you got it', 'yep', 'sure', 'okay', 'all right'] \n",
    "sequence_closer = ['awesome','great','perfect','exactly','that\\'s all','that is all', 'thanks', 'thank you', 'pleasure', 'excellent', 'okay', 'excellent', 'too bad', 'oh well']\n",
    "inquiry = ['\\?']\n",
    "receipt = ['You are welcome', 'you\\'re welcome']\n",
    "disconfirmation = [' no ', 'wrong', 'incorrect', 'not really']\n",
    "\n",
    "## !!!!! order-sensitive !!!!\n",
    "\n",
    "#this variable chooses only one label, rules goes on order. once it assigns one rule it won't check next\n",
    "temp2=df.new_text.fillna(\"0\")\n",
    "df['DA_indicators_sent'] =  np.where(temp2.str.contains('|'.join(repair_initiator), case=False), \"repair_initiator\",\n",
    "                            np.where(temp2.str.contains('|'.join(greeting), case=False), \"greeting\",\n",
    "                            np.where(temp2.str.contains('|'.join(request_summary), case=False),  \"request_summary\",\n",
    "                            np.where(temp2.str.contains('|'.join(confirmation), case=False),  \"confirmation\",\n",
    "                            np.where(temp2.str.contains('|'.join(receipt), case=False),  \"receipt\",\n",
    "                            np.where(temp2.str.contains('|'.join(disconfirmation), case=False),  \"disconfirmation\",\n",
    "                            np.where(temp2.str.contains('|'.join(inquiry), case=False),  \"inquiry\",\n",
    "                            np.where(temp2.str.contains('|'.join(repair), case=False),  \"repair\",\n",
    "                            np.where(temp2.str.contains('|'.join(sequence_closer), case=False), \"sequence_closer\", \"x\")))))))))\n",
    "\n",
    "print(df['DA_indicators_sent'].value_counts())\n",
    "\n",
    "#it works! #binary variables and in the end gather in one new column\n",
    "##### ainda n sei se repair solo fica ######                                     \n",
    "                                     \n",
    "df['DA_rep_init'] =  np.where(temp2.str.contains('|'.join(repair_initiator), case=False), \"repair_initiator\", '')\n",
    "df['DA_rep'] =  np.where(temp2.str.contains('|'.join(repair), case=False), \"repair\", '')\n",
    "df['DA_greet'] =  np.where(temp2.str.contains('|'.join(greeting), case=False), \"greeting\", '')\n",
    "df['DA_req_sum'] =  np.where(temp2.str.contains('|'.join(request_summary), case=False), \"request_summary\", '')\n",
    "df['DA_conf'] =  np.where(temp2.str.contains('|'.join(confirmation), case=False), \"confirmation\", '')\n",
    "df['DA_receipt'] =  np.where(temp2.str.contains('|'.join(receipt), case=False), \"receipt\", '')\n",
    "df['DA_disconf'] =  np.where(temp2.str.contains('|'.join(disconfirmation), case=False), \"disconfirmation\", '')\n",
    "df['DA_inquiry'] =  np.where(temp2.str.contains('|'.join(inquiry), case=False), \"inquiry\", '')\n",
    "df['DA_closer'] =  np.where(temp2.str.contains('|'.join(sequence_closer), case=False), \"sequence_closer\", '')\n",
    "df['all_DA'] = df[['DA_rep_init', 'DA_rep', 'DA_greet','DA_req_sum', 'DA_conf', 'DA_receipt', 'DA_disconf', 'DA_inquiry', 'DA_closer']].agg(' '.join, axis=1)\n",
    "\n",
    "df['all_DA'] = [word_tokenize(da) for da in df['all_DA']]\n",
    "\n",
    "df.loc[:, ['speaker', 'new_text', 'all_DA']].to_csv('analysis_multiple_DA_cases_2nditeration.csv')\n",
    "df.loc[:, ['speaker', 'new_text', 'all_DA']]\n",
    "counts = df['all_DA'].value_counts()\n",
    "\n",
    "counts.to_csv('count_overlaping_DAs_2nditeration.csv')\n",
    "data = {'counts':counts}\n",
    "df_counts = pd.DataFrame(data)\n",
    "df_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea1b246",
   "metadata": {},
   "source": [
    "## Third iteration and FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49963883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>[]</th>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[sequence_closer]</th>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[confirmation, sequence_closer]</th>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[confirmation]</th>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[greeting]</th>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 counts\n",
       "[]                                  700\n",
       "[sequence_closer]                   204\n",
       "[confirmation, sequence_closer]     182\n",
       "[confirmation]                      160\n",
       "[greeting]                          158"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DA indicators. Obs: the vars bellow are sensitive to spacing !!! 'hi' != ' hi'\n",
    "\n",
    "greeting = [' hi','hi.','hello','yo ', 'hey', 'How can I help you?'] \n",
    "repair_initiator = ['sorry','repeat', 'understand', 'mean ', 'what?', 'example?', 'could you say that again?', 'say again', 'what did you say?', 'I can\\'t hear you', 'I didn\\'t listen', 'say it again']\n",
    "repair = ['I meant']\n",
    "request_summary = [' correct?','confirm']  \n",
    "confirmation = ['yes',' correct.',' correct ', 'that\\'s it', 'indeed', 'yeah', 'that\\'s right', 'you got it', 'yep', 'sure', 'okay', 'all right',  'sure', 'sounds good', 'super.', 'super!', 'alright', 'I don\\'t mind', 'I can help you', 'sounds really good', 'got it.'] \n",
    "sequence_closer = ['awesome','great','perfect','exactly','that\\'s all','that is all', 'thanks', 'thank you', 'pleasure', 'excellent', 'okay', 'excellent', 'too bad', 'oh well', 'have a good day', 'enjoy', 'until next time', 'good day', 'good luck', 'bye', 'til next time']\n",
    "receipt = ['You are welcome', 'you\\'re welcome']\n",
    "disconfirmation = [' no ', 'wrong', 'incorrect', 'not really']\n",
    "#NEW\n",
    "completion_check = ['is that all?','anything else I can help you with?', 'anything else?'] \n",
    "partial_request = ['\\?']\n",
    "detail_request = ['\\?']\n",
    "hold_request = ['one moment, please',' hold on', 'hold', 'just a moment', 'let me check for you', 'let me see', 'one sec' ]\n",
    "\n",
    "temp2=df.new_text.fillna(\"0\")\n",
    "\n",
    "## !!!!! order-sensitive !!!!\n",
    "\n",
    "# #this variable chooses only one label, rules goes on order. once it assigns one rule it won't check next\n",
    "\n",
    "# df['DA_indicators_sent'] =  np.where(temp2.str.contains('|'.join(repair_initiator), case=False), \"repair_initiator\",\n",
    "#                             np.where(temp2.str.contains('|'.join(greeting), case=False), \"greeting\",\n",
    "#                             np.where(temp2.str.contains('|'.join(request_summary), case=False),  \"request_summary\",\n",
    "#                             np.where(temp2.str.contains('|'.join(confirmation), case=False),  \"confirmation\",\n",
    "#                             np.where(temp2.str.contains('|'.join(receipt), case=False),  \"receipt\",\n",
    "#                             np.where(temp2.str.contains('|'.join(disconfirmation), case=False),  \"disconfirmation\",\n",
    "#                             np.where(temp2.str.contains('|'.join(repair), case=False),  \"repair\",\n",
    "#                             np.where(temp2.str.contains('|'.join(sequence_closer), case=False), \"sequence_closer\", \n",
    "#                             np.where(temp2.str.contains('|'.join(completion_check), case=False),  \"completion_check\",\n",
    "#                             np.where(temp2.str.contains('|'.join(hold_request), case=False),  \"hold_request\",\n",
    "#                             np.where(temp2.str.contains('|'.join(partial_request), case=False),  \"partial_request\",\n",
    "#                             np.where(temp2.str.contains('|'.join(detail_request), case=False),  \"detail_request\",\"x\"))))))))))))\n",
    "\n",
    "# print(df['DA_indicators_sent'].value_counts())\n",
    "\n",
    "\n",
    "#it works! #binary variables and in the end gather in one new column\n",
    "##### ainda n sei se repair solo fica ######                                     \n",
    "                                     \n",
    "df['DA_rep_init'] =  np.where(temp2.str.contains('|'.join(repair_initiator), case=False), \"repair_initiator\", '')\n",
    "df['DA_rep'] =  np.where(temp2.str.contains('|'.join(repair), case=False), \"repair\", '')\n",
    "df['DA_greet'] =  np.where(temp2.str.contains('|'.join(greeting), case=False), \"greeting\", '')\n",
    "df['DA_req_sum'] =  np.where(temp2.str.contains('|'.join(request_summary), case=False), \"request_summary\", '')\n",
    "df['DA_conf'] =  np.where(temp2.str.contains('|'.join(confirmation), case=False), \"confirmation\", '')\n",
    "df['DA_receipt'] =  np.where(temp2.str.contains('|'.join(receipt), case=False), \"receipt\", '')\n",
    "df['DA_disconf'] =  np.where(temp2.str.contains('|'.join(disconfirmation), case=False), \"disconfirmation\", '')\n",
    "df['DA_closer'] =  np.where(temp2.str.contains('|'.join(sequence_closer), case=False), \"sequence_closer\", '')           \n",
    "df['DA_comp_check'] =  np.where(temp2.str.contains('|'.join(completion_check), case=False),  \"completion_check\", '')\n",
    "df['DA_hold'] =  np.where(temp2.str.contains('|'.join(hold_request), case=False),  \"hold_request\", '')\n",
    "df['DA_partial_req'] = np.where(((df['speaker'] == 'USER') & temp2.str.contains('|'.join(partial_request), case=False)), 'partial_request', '')\n",
    "df['DA_detail_req'] = np.where(((df['speaker'] == 'ASSISTANT') & temp2.str.contains('|'.join(detail_request), case=False)), 'detail_request', '')\n",
    "                                                            \n",
    "                                     \n",
    "df['all_DA'] = df[['DA_rep_init', 'DA_rep', 'DA_greet','DA_req_sum', 'DA_conf', 'DA_receipt', 'DA_disconf', 'DA_closer', 'DA_comp_check', 'DA_hold', 'DA_partial_req', 'DA_detail_req']].agg(' '.join, axis=1)\n",
    "\n",
    "df['all_DA'] = [word_tokenize(da) for da in df['all_DA']]\n",
    "\n",
    "df.loc[:, ['speaker', 'new_text', 'all_DA']].to_csv('analysis_multiple_DA_cases_2nditeration.csv')\n",
    "df.loc[:, ['speaker', 'new_text', 'all_DA']]\n",
    "counts = df['all_DA'].value_counts()\n",
    "\n",
    "counts.to_csv('count_overlaping_DAs_2nditeration.csv')\n",
    "data = {'counts':counts}\n",
    "df_counts = pd.DataFrame(data)\n",
    "df_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429a4741",
   "metadata": {},
   "source": [
    "### Compare binary against binary. Kappa score\n",
    "#### I think it makes no sense to compare the multiple labels together because if only one is different it gives a zero for all\n",
    "\n",
    "The kappa statistic, which is a number between -1 and 1. The maximum value means complete agreement; zero or lower means chance agreement.\n",
    "\n",
    "Conclusion: remove request summary, examples are not matching. Also I don't see any easy to implement rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0b223c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kappa: 0.4594313518154099\n",
      "kappa if eliminate req sum: 0.5015399877265041\n",
      "kappa if eliminate req sum and repair: 0.5516939864991545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('repair_initiator', 0.1380632187519637),\n",
       " ('repair', 0.0),\n",
       " ('greeting', 0.44362157087722953),\n",
       " ('request_summary', -0.0037636432066239234),\n",
       " ('confirmation', 0.7218074236332794),\n",
       " ('receipt', 0.9081041968162084),\n",
       " ('disconfirmation', 0.07849785129678011),\n",
       " ('sequence_closer', 0.43044070083803143),\n",
       " ('completion_check', 0.7030793367582366),\n",
       " ('hold_request', 0.6719566365966774),\n",
       " ('partial_request', 0.6439770267947266),\n",
       " ('detail_request', 0.7773919026284104)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.iloc[:,[9,22]]\n",
    "\n",
    "DA = ['repair_initiator','repair','greeting','request_summary','confirmation','receipt','disconfirmation',\n",
    "      'sequence_closer','completion_check','hold_request','partial_request', 'detail_request'] \n",
    "kappa = []\n",
    "\n",
    "for e,i in enumerate(range(9,21)):\n",
    "    gold = np.asarray([0 if val == DA[e] else 1 for val in df.iloc[:, i]])\n",
    "    synt = np.asarray([0 if val == DA[e] else 1 for val in df.iloc[:, (i+13)]])\n",
    "    kappa.append(sklearn.metrics.cohen_kappa_score(gold, synt))                  \n",
    "\n",
    "kappa_scores = list(zip(DA,kappa))\n",
    "\n",
    "print('kappa: ' + str(np.mean(kappa)))\n",
    "print('kappa if eliminate req sum: '+str((sum(kappa)-kappa[3])/(len(kappa)-1)))\n",
    "print('kappa if eliminate req sum and repair: '+str((sum(kappa)-kappa[3]-kappa[1])/(len(kappa)-2)))\n",
    "kappa_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890e096c",
   "metadata": {},
   "source": [
    "## Now sample a part to annotate\n",
    "### criteria:\n",
    "\n",
    "- take from all taks\n",
    "- annotate full dialogues\n",
    "- how many dialogues are there? what's the percentage of full dialogues i want to annotate?\n",
    "17304 dialogues (index 0 demarks the begining of a dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db5b2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of dialogues:\n",
    "df_count = pd.read_csv('./data_TM2/concatenated_tasks.csv', index_col=0)\n",
    "print(df_count['conversation_id'].nunique())\n",
    "\n",
    "#average size of dialog\n",
    "print(df_count['index'].mean())\n",
    "\n",
    "#dialogues per task (i.e True column)\n",
    "pd.crosstab(df_count['task'], df_count['index'] ==0).sort_values(by='task', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e87bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample randomly 167 dialogues, that will be split into 2 batches:\n",
    "#selection made using the unique conversation_ids\n",
    "\n",
    "np.random.seed(0)\n",
    "unique_conv_id = df['conversation_id'].unique()\n",
    "sample_ids = np.random.choice(unique_conv_id, 172, replace=False)\n",
    "\n",
    "sample_b1  = []\n",
    "sample_b2  = []\n",
    "for e, row in enumerate(df['conversation_id']):\n",
    "    for b1 in sample_ids[:int(len(sample_ids)/2)]:\n",
    "        if row == b1:\n",
    "            sample_b1.append(df.loc[e,:])\n",
    "    for b2 in sample_ids[int(len(sample_ids)/2):]:\n",
    "        if row == b2:\n",
    "            sample_b2.append(df.loc[e,:])\n",
    "\n",
    "df_sample_b1 = pd.DataFrame(sample_b1)\n",
    "df_sample_b2 = pd.DataFrame(sample_b2)\n",
    "\n",
    "df_sample_b1.to_csv('./data_TM2/sample_b1.csv')\n",
    "df_sample_b2.to_csv('./data_TM2/sample_b2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2331b7",
   "metadata": {},
   "source": [
    "## Getting insights from k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dbb51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df['new_text'])\n",
    "\n",
    "true_k = 5\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "for i in range(true_k):\n",
    "    print('Cluster %d:' % i),\n",
    "    for ind in order_centroids[i, :5]:\n",
    "        print(' %s' % terms[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df3fdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Prediction')\n",
    "X = vectorizer.transform(['hi, can you help me?'])\n",
    "predicted = model.predict(X)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26edefe5",
   "metadata": {},
   "source": [
    "## Repair cases\n",
    "\n",
    "Next round to detecting misunderstanding repair happens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ca63d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS WORKS! need to apply all filters\n",
    "# df.loc[df['column name'] condition, 'new column name'] = 'value if condition is met'\n",
    "\n",
    "df.loc[(df['speaker'] != df['speaker'].shift(1)) & (df['DA_indicators_sent'].shift(1) == 'repair_initiator'), 'teste_col2'] = 'SECOND'\n",
    "\n",
    "df['teste_col2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7491b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['teste_col2'] == 'SECOND']\n",
    "#second is more an ANSWER (i.e. next round) to: sorry, what did you say? OR what did you mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1ddda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('a case of self repair: ')\n",
    "print(df.iloc[1708,4:-1])\n",
    "print('\\n\\n')\n",
    "print(df.iloc[1709,4:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853737d6",
   "metadata": {},
   "source": [
    "## Cosine similarity for paraphrasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46ea70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= 'he is the king of the world'\n",
    "b= 'His majesty rules the planet'\n",
    "c= 'I love my dog'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0617ba87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
