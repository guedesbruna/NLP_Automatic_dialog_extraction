{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de9d64ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.io.json import json_normalize\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import sklearn\n",
    "\n",
    "import os\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import smart_open\n",
    "\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "from sklearn.metrics import pairwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478c38d6",
   "metadata": {},
   "source": [
    "## Load whole dataset to work with DA patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c635da6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('./data_TM2/synt_annotated_data.csv', index_col=0)\n",
    "df = pd.read_csv('./data_TM2/processed/processed_utterances_sentence_DA_labeling.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b14a95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>instruction_id</th>\n",
       "      <th>index</th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "      <th>segments</th>\n",
       "      <th>new_text</th>\n",
       "      <th>DA_rep_init</th>\n",
       "      <th>DA_greet</th>\n",
       "      <th>...</th>\n",
       "      <th>DA_receipt</th>\n",
       "      <th>DA_disconf</th>\n",
       "      <th>DA_closer</th>\n",
       "      <th>DA_comp_check</th>\n",
       "      <th>DA_hold</th>\n",
       "      <th>DA_partial_req</th>\n",
       "      <th>DA_detail_req</th>\n",
       "      <th>DA_grant</th>\n",
       "      <th>DA_answer</th>\n",
       "      <th>all_DA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>flights</td>\n",
       "      <td>dlg-00100680-00e0-40fe-8321-6d81b21bfc4f</td>\n",
       "      <td>flight-12</td>\n",
       "      <td>0</td>\n",
       "      <td>U</td>\n",
       "      <td>Hello. I'd like to find a round trip commercia...</td>\n",
       "      <td>[{'start_index': 26, 'end_index': 36, 'text': ...</td>\n",
       "      <td>Hello.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>U_greeting</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['U_greeting']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flights</td>\n",
       "      <td>dlg-00100680-00e0-40fe-8321-6d81b21bfc4f</td>\n",
       "      <td>flight-12</td>\n",
       "      <td>0</td>\n",
       "      <td>U</td>\n",
       "      <td>Hello. I'd like to find a round trip commercia...</td>\n",
       "      <td>[{'start_index': 26, 'end_index': 36, 'text': ...</td>\n",
       "      <td>I'd like to find a round trip commercial airli...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>flights</td>\n",
       "      <td>dlg-00100680-00e0-40fe-8321-6d81b21bfc4f</td>\n",
       "      <td>flight-12</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>Hello, how can I help you?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hello, how can I help you?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A_greeting</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A_detail_request</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['A_greeting', 'A_detail_request']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>flights</td>\n",
       "      <td>dlg-00100680-00e0-40fe-8321-6d81b21bfc4f</td>\n",
       "      <td>flight-12</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>San Francisco to Denver, got it.</td>\n",
       "      <td>[{'start_index': 0, 'end_index': 13, 'text': '...</td>\n",
       "      <td>San Francisco to Denver, got it.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['A_confirmation']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>flights</td>\n",
       "      <td>dlg-00100680-00e0-40fe-8321-6d81b21bfc4f</td>\n",
       "      <td>flight-12</td>\n",
       "      <td>3</td>\n",
       "      <td>U</td>\n",
       "      <td>You're really on top of things. I like that.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>You're really on top of things.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>U_greeting</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['U_greeting']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      task                           conversation_id instruction_id  index  \\\n",
       "0  flights  dlg-00100680-00e0-40fe-8321-6d81b21bfc4f      flight-12      0   \n",
       "1  flights  dlg-00100680-00e0-40fe-8321-6d81b21bfc4f      flight-12      0   \n",
       "2  flights  dlg-00100680-00e0-40fe-8321-6d81b21bfc4f      flight-12      1   \n",
       "3  flights  dlg-00100680-00e0-40fe-8321-6d81b21bfc4f      flight-12      2   \n",
       "4  flights  dlg-00100680-00e0-40fe-8321-6d81b21bfc4f      flight-12      3   \n",
       "\n",
       "  speaker                                               text  \\\n",
       "0       U  Hello. I'd like to find a round trip commercia...   \n",
       "1       U  Hello. I'd like to find a round trip commercia...   \n",
       "2       A                         Hello, how can I help you?   \n",
       "3       A                   San Francisco to Denver, got it.   \n",
       "4       U       You're really on top of things. I like that.   \n",
       "\n",
       "                                            segments  \\\n",
       "0  [{'start_index': 26, 'end_index': 36, 'text': ...   \n",
       "1  [{'start_index': 26, 'end_index': 36, 'text': ...   \n",
       "2                                                NaN   \n",
       "3  [{'start_index': 0, 'end_index': 13, 'text': '...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                            new_text DA_rep_init    DA_greet  \\\n",
       "0                                             Hello.         NaN  U_greeting   \n",
       "1  I'd like to find a round trip commercial airli...         NaN         NaN   \n",
       "2                         Hello, how can I help you?         NaN  A_greeting   \n",
       "3                   San Francisco to Denver, got it.         NaN         NaN   \n",
       "4                    You're really on top of things.         NaN  U_greeting   \n",
       "\n",
       "   ... DA_receipt DA_disconf DA_closer DA_comp_check DA_hold DA_partial_req  \\\n",
       "0  ...        NaN        NaN       NaN           NaN     NaN            NaN   \n",
       "1  ...        NaN        NaN       NaN           NaN     NaN            NaN   \n",
       "2  ...        NaN        NaN       NaN           NaN     NaN            NaN   \n",
       "3  ...        NaN        NaN       NaN           NaN     NaN            NaN   \n",
       "4  ...        NaN        NaN       NaN           NaN     NaN            NaN   \n",
       "\n",
       "      DA_detail_req DA_grant DA_answer                              all_DA  \n",
       "0               NaN      NaN       NaN                      ['U_greeting']  \n",
       "1               NaN      NaN       NaN                                  []  \n",
       "2  A_detail_request      NaN       NaN  ['A_greeting', 'A_detail_request']  \n",
       "3               NaN      NaN       NaN                  ['A_confirmation']  \n",
       "4               NaN      NaN       NaN                      ['U_greeting']  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3cf606",
   "metadata": {},
   "source": [
    "# doc2vec working\n",
    "https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html#sphx-glr-auto-examples-tutorials-run-doc2vec-lee-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e656f204",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df,train_size=393180, shuffle=False) #split taking into account full dialogue. Not splitting in middle of dialogue\n",
    "train, val = train_test_split(train,train_size=0.1, shuffle=False) #sÃ³ pra testar coisas 0.001 funciona bem! 0.01 20s "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efda9f9",
   "metadata": {},
   "source": [
    "Below, we define a function to:\n",
    "\n",
    "- open the train/test file (with latin encoding)\n",
    "\n",
    "- read the file line-by-line\n",
    "\n",
    "- pre-process each line (tokenize text into individual words, remove punctuation, set to lowercase, etc)\n",
    "\n",
    "The file weâ€™re reading is a corpus. Each line of the file is a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abfb958f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['what', 'is', 'the', 'latest', 'headlines', 'related', 'to', 'the', 'philadelphia', 'phillies'], ['series', 'preview', 'philadelphia', 'phillies', 'at', 'milwaukee', 'brewers', 'mlb', 'rumors', 'latest', 'sign', 'phillies', 'will', 'make', 'full', 'court', 'press', 'for', 'manny', 'machado']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43703"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_corpus(fname, tokens_only=False):\n",
    "        for i, line in enumerate(fname): \n",
    "            tokens = gensim.utils.simple_preprocess(line)\n",
    "            if tokens_only:\n",
    "                yield tokens\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "\n",
    "train_corpus = list(read_corpus(train['new_text']))\n",
    "test_corpus = list(read_corpus(test['new_text'], tokens_only=True))\n",
    "\n",
    "#training corpus one instance example\n",
    "print(test_corpus[:2])\n",
    "len(test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dee46b",
   "metadata": {},
   "source": [
    "Training the Model\n",
    "\n",
    "Now, weâ€™ll instantiate a Doc2Vec model with a vector size with 50 dimensions and iterating over the training corpus 40 times. We set the minimum word count to 2 in order to discard words with very few occurrences. (Without a variety of representative examples, retaining such infrequent words can often make a model worse!) Typical iteration counts in the published Paragraph Vector paper results, using 10s-of-thousands to millions of docs, are 10-20. More iterations take more time and eventually reach a point of diminishing returns.\n",
    "\n",
    "However, this is a very very small dataset (300 documents) with shortish documents (a few hundred words). Adding training passes can sometimes help with such small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f047756d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=80)\n",
    "\n",
    "#Build a vocabulary\n",
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26024796",
   "metadata": {},
   "source": [
    "Essentially, the vocabulary is a list (accessible via model.wv.index_to_key) of all of the unique words extracted from the training corpus. \n",
    "\n",
    "Additional attributes for each word are available using the model.wv.get_vecattr() method \n",
    "\n",
    "For example, to see how many times penalty appeared in the training corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b9cbff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'hello' appeared 625 times in the training corpus.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "625"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Word 'hello' appeared {model.wv.get_vecattr('hello', 'count')} times in the training corpus.\")\n",
    "\n",
    "model.wv.get_vecattr('hello', 'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d0a364",
   "metadata": {},
   "source": [
    "Next, train the model on the corpus. If optimized Gensim (with BLAS library) is being used, this should take no more than 3 seconds. If the BLAS library is not being used, this should take no more than 2 minutes, so use optimized Gensim with BLAS if you value your time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c95ca6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 2s, sys: 27.6 s, total: 2min 29s\n",
      "Wall time: 2min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8eaa4c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = get_tmpfile(\"my_doc2vec_model\")\n",
    "\n",
    "model.save(fname)\n",
    "model = gensim.models.doc2vec.Doc2Vec.load(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ad3ddd",
   "metadata": {},
   "source": [
    "## Similarity window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "646ac80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['unique'] = [n for n in range(len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({nan: 434770, 'A_repair_initiator': 1562, 'U_repair_initiator': 551})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'nan': 434770, 'repair_initiator': 2113})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "print(collections.Counter(df['DA_rep_init']))\n",
    "repair_initiator = ['A_repair_initiator', 'U_repair_initiator' ]\n",
    "temp2=df.DA_rep_init.fillna(\"0\")\n",
    "df['DA_rep_init_new'] =  np.where(temp2.str.contains('|'.join(repair_initiator), case=False), \"repair_initiator\", np.nan)\n",
    "collections.Counter(df['DA_rep_init_new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0facbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrasing(conversation):\n",
    "    '''Output either similarity or paraphrase.\n",
    "    paraphrase is the second sentence said in a sequence of time.\n",
    "    similarity gives the pairs of similarities with its cosines.\n",
    "    Each tuple contains: index_sent_1, sent1, index_sent2, sent2, cosine_similarity\n",
    "    '''\n",
    "    \n",
    "    sentences_window = {}\n",
    "    nrows = len(conversation['new_text'])\n",
    "    sentences = conversation['new_text']\n",
    "    rep_initiators = np.array(conversation['DA_rep_init_new'])\n",
    "    window_width = 2\n",
    "    dialog_id = conversation['conversation_id'].unique()[0]\n",
    "\n",
    "    for k,sentence,rep_init in zip(range(nrows),sentences,rep_initiators):\n",
    "        if rep_init== \"repair_initiator\":\n",
    "            sentences_window[sentence] = sentences[k-window_width:k+window_width+1]\n",
    "\n",
    "\n",
    "    rep=pd.DataFrame(sentences_window)\n",
    "    repair_tok = list(read_corpus(rep.iloc[:,0], tokens_only=True))\n",
    "\n",
    "    inf_vecs = []\n",
    "    [inf_vecs.append(model.infer_vector(ut)) for ut in repair_tok]  \n",
    "\n",
    "\n",
    "    threshold = 0.7\n",
    "    similarity = []\n",
    "    paraphrase = []\n",
    "\n",
    "    for e in range(len(inf_vecs)):\n",
    "        counter = e+1\n",
    "        while counter < len(inf_vecs):\n",
    "            pair_sim = pairwise.cosine_similarity(inf_vecs[e].reshape(1, -1), inf_vecs[counter].reshape(1, -1))\n",
    "            if pair_sim >= threshold:\n",
    "                similarity.append((e, rep.iloc[e,0],counter, rep.iloc[counter,0],pair_sim)) #((e,counter, pair_sim))\n",
    "                paraphrase.append(rep.iloc[counter,0])\n",
    "            counter +=1\n",
    "\n",
    "    return similarity #paraphrase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a5b3a1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219\n",
      "1813\n"
     ]
    }
   ],
   "source": [
    "def match_similarity_w_dialogueid(df):\n",
    "    \n",
    "    #select dialogue ids with repair initiators\n",
    "    dial_w_rep = []\n",
    "    for row in range(len(df)):\n",
    "        if df['DA_rep_init_new'][row] == 'repair_initiator':\n",
    "            dial_w_rep.append(df['conversation_id'][row])\n",
    "    dial_w_rep = list(set(dial_w_rep))\n",
    "\n",
    "    #call function for all dialogues with repair initiator. save in dict in which key=dialogue id and value=paraphrase\n",
    "    paraphr = {}\n",
    "    count =0\n",
    "    for dialogue in dial_w_rep:\n",
    "        try:\n",
    "            conversation = df.loc[df['conversation_id'] == dialogue]\n",
    "            paraphr[dialogue] = paraphrasing(conversation)\n",
    "        except Exception:\n",
    "            count+=1\n",
    "            pass\n",
    "\n",
    "    print(count) #number of times it could not compute the similarity function\n",
    "    print(len(dial_w_rep))\n",
    "\n",
    "    return paraphr\n",
    "\n",
    "paraphrasing = match_similarity_w_dialogueid(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2db5357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open('./src/generated_files/paraphrasing_cos_sim07.pkl', 'wb') as f:\n",
    "    pickle.dump(paraphrasing, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983dc113",
   "metadata": {},
   "source": [
    "#### Now add similarity as a column in dataframe.\n",
    "\n",
    "#### !!! This is not yet working since it generates more examples than the corresponding lines in dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b0af35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #create column in dataframe with rephrase\n",
    "\n",
    "# DA_paraphrase = []\n",
    "\n",
    "# # for key, value in paraphr.items():\n",
    "# #     if len(value) != 0:\n",
    "# # #         print(type(value[0]))\n",
    "# # #         print(key, value, len(value))\n",
    "# #         for v in value:\n",
    "# # #             print(key, v)\n",
    "# # #             df['DA_paraphrase'] = np.where(((df['conversation_id'] == key) & (df['new_text'] == v)), 'Paraphrase', '')                                     \n",
    "# #             for row in range(len(df)):\n",
    "# #                 if df['conversation_id'][row] == key & df['new_text'][row] == v:\n",
    "# #                     DA_paraphrase.append(1)\n",
    "# #                 else:\n",
    "# #                     DA_paraphrase.append(0)\n",
    "\n",
    "# for row in range(len(df)):\n",
    "#     for key, value in paraphr.items():\n",
    "#         if len(value) != 0:\n",
    "#             if (df['conversation_id'][row] == key) & (df['new_text'][row] == max(value)):\n",
    "#                 DA_paraphrase.append(1)\n",
    "#         else:\n",
    "#             DA_paraphrase.append(0)\n",
    "\n",
    "# df['DA_paraphrase'] = DA_paraphrase\n",
    "\n",
    "# (df['conversation_id'][0] == 'dlg-00100680-00e0-40fe-8321-6d81b21bfc4f') & (df['new_text'][0] == 'Hello.')\n",
    "\n",
    "# len(DA_paraphrase)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ed6b9b66afc0387774ca8c164c16421c0425c6a8fbcef042df40463fb3616e61"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('inteamenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
