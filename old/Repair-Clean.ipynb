{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de9d64ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.io.json import json_normalize\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478c38d6",
   "metadata": {},
   "source": [
    "## Load whole dataset to work with DA patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c635da6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data_TM2/synt_annotated_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3cf606",
   "metadata": {},
   "source": [
    "# doc2vec working\n",
    "https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html#sphx-glr-auto-examples-tutorials-run-doc2vec-lee-py\n",
    "\n",
    "training our model using the Lee Background Corpus included in gensim. This corpus contains 314 documents selected from the Australian Broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e656f204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import smart_open\n",
    "\n",
    "\n",
    "train, test = train_test_split(df,train_size=393180, shuffle=False) #split taking into account full dialogue. Not splitting in middle of dialogue\n",
    "train, val = train_test_split(train,train_size=0.1, shuffle=False) #só pra testar coisas 0.001 funciona bem! 0.01 20s "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efda9f9",
   "metadata": {},
   "source": [
    "Below, we define a function to:\n",
    "\n",
    "- open the train/test file (with latin encoding)\n",
    "\n",
    "- read the file line-by-line\n",
    "\n",
    "- pre-process each line (tokenize text into individual words, remove punctuation, set to lowercase, etc)\n",
    "\n",
    "The file we’re reading is a corpus. Each line of the file is a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abfb958f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(fname, tokens_only=False):\n",
    "#     with smart_open.open(fname, encoding=\"iso-8859-1\") as f: #### FOR MY CODE COMMENT THIS LINE #####\n",
    "        for i, line in enumerate(fname): #### FOR MY CODE PUT (fname) ##### default is (f)\n",
    "            tokens = gensim.utils.simple_preprocess(line)\n",
    "            if tokens_only:\n",
    "                yield tokens\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "\n",
    "# train_corpus = list(read_corpus(lee_train_file))\n",
    "# test_corpus = list(read_corpus(lee_test_file, tokens_only=True))\n",
    "\n",
    "train_corpus = list(read_corpus(train['new_text']))\n",
    "test_corpus = list(read_corpus(test['new_text'], tokens_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "711a5787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['what', 'is', 'the', 'latest', 'headlines', 'related', 'to', 'the', 'philadelphia', 'phillies'], ['series', 'preview', 'philadelphia', 'phillies', 'at', 'milwaukee', 'brewers', 'mlb', 'rumors', 'latest', 'sign', 'phillies', 'will', 'make', 'full', 'court', 'press', 'for', 'manny', 'machado']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43703"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training corpus one instance example\n",
    "print(test_corpus[:2])\n",
    "len(test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dee46b",
   "metadata": {},
   "source": [
    "Training the Model\n",
    "\n",
    "Now, we’ll instantiate a Doc2Vec model with a vector size with 50 dimensions and iterating over the training corpus 40 times. We set the minimum word count to 2 in order to discard words with very few occurrences. (Without a variety of representative examples, retaining such infrequent words can often make a model worse!) Typical iteration counts in the published Paragraph Vector paper results, using 10s-of-thousands to millions of docs, are 10-20. More iterations take more time and eventually reach a point of diminishing returns.\n",
    "\n",
    "However, this is a very very small dataset (300 documents) with shortish documents (a few hundred words). Adding training passes can sometimes help with such small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f047756d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=80)\n",
    "\n",
    "#Build a vocabulary\n",
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26024796",
   "metadata": {},
   "source": [
    "Essentially, the vocabulary is a list (accessible via model.wv.index_to_key) of all of the unique words extracted from the training corpus. \n",
    "\n",
    "Additional attributes for each word are available using the model.wv.get_vecattr() method \n",
    "\n",
    "For example, to see how many times penalty appeared in the training corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b9cbff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'hello' appeared 625 times in the training corpus.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "625"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Word 'hello' appeared {model.wv.get_vecattr('hello', 'count')} times in the training corpus.\")\n",
    "\n",
    "model.wv.get_vecattr('hello', 'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d0a364",
   "metadata": {},
   "source": [
    "Next, train the model on the corpus. If optimized Gensim (with BLAS library) is being used, this should take no more than 3 seconds. If the BLAS library is not being used, this should take no more than 2 minutes, so use optimized Gensim with BLAS if you value your time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95ca6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaa4c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "fname = get_tmpfile(\"my_doc2vec_model\")\n",
    "\n",
    "model.save(fname)\n",
    "model = gensim.models.doc2vec.Doc2Vec.load(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ad3ddd",
   "metadata": {},
   "source": [
    "## Clean code for Window part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a22cb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise\n",
    "\n",
    "\n",
    "def paraphrasing(conversation):\n",
    "    sentences_window = {}\n",
    "    nrows = len(conversation['new_text'])\n",
    "    sentences = conversation['new_text']\n",
    "    rep_initiators = np.array(conversation['DA_rep_init'])\n",
    "    window_width = 2\n",
    "    dialog_id = conversation['conversation_id'].unique()[0]\n",
    "\n",
    "    for k,sentence,rep_init in zip(range(nrows),sentences,rep_initiators):\n",
    "        if rep_init== \"repair_initiator\":\n",
    "            sentences_window[sentence] = sentences[k-window_width:k+window_width+1]\n",
    "\n",
    "\n",
    "    rep=pd.DataFrame(sentences_window)#.transpose()\n",
    "    repair_tok = list(read_corpus(rep.iloc[:,0], tokens_only=True))\n",
    "\n",
    "\n",
    "    inf_vecs = []\n",
    "    [inf_vecs.append(model.infer_vector(ut)) for ut in repair_tok]  \n",
    "\n",
    "\n",
    "    threshold = 0.3\n",
    "    paraphrasing = []\n",
    "    paraph = {}\n",
    "\n",
    "    for e in range(len(inf_vecs)):\n",
    "        counter = e+1\n",
    "        while counter < len(inf_vecs):\n",
    "            pair_sim = pairwise.cosine_similarity(inf_vecs[e].reshape(1, -1), inf_vecs[counter].reshape(1, -1))\n",
    "            if pair_sim >= threshold:\n",
    "                paraphrasing.append((e, rep.iloc[e,0],counter, rep.iloc[counter,0],pair_sim)) #((e,counter, pair_sim))#\n",
    "            counter +=1\n",
    "    paraph[dialog_id] = paraphrasing\n",
    "\n",
    "    return paraphrasing # paraph \n",
    "    #dict in which key in dialogue number and values are tuples that passes threshold. \n",
    "    #Each tuple contains: index_sent_1, sent1, index_sent2, sent2, cosine_similarity\n",
    "\n",
    "#works! selects dialogue ids with repair initiators\n",
    "\n",
    "dial_w_rep = []\n",
    "for row in range(len(df)):\n",
    "    if df['DA_rep_init'][row] == 'repair_initiator':\n",
    "        dial_w_rep.append(df['conversation_id'][row])\n",
    "dial_w_rep = list(set(dial_w_rep))\n",
    "\n",
    "#call function for all dialogues w repair and save in dict in which key = dialogue id and value is the cosines\n",
    "\n",
    "paraphr = {}\n",
    "count =0\n",
    "for dialogue in dial_w_rep:\n",
    "    try:\n",
    "        conversation = df.loc[df['conversation_id'] == dialogue]\n",
    "        paraphr[dialogue] = paraphrasing(conversation)\n",
    "    except Exception:\n",
    "        count+=1\n",
    "        pass\n",
    "    \n",
    "print(count)\n",
    "print(len(dial_w_rep)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda0465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import pairwise\n",
    "\n",
    "# conversation1 = df.loc[df['conversation_id'] == 'dlg-00e32998-0b0f-47f1-a4f0-2ce90f1718d0']\n",
    "\n",
    "# sentences_window = {}\n",
    "# nrows = len(conversation1['new_text'])\n",
    "# sentences = conversation1['new_text']\n",
    "# rep_initiators = np.array(conversation1['DA_rep_init'])\n",
    "# window_width = 2\n",
    "\n",
    "# for k,sentence,rep_init in zip(range(nrows),sentences,rep_initiators):\n",
    "#     if rep_init== \"repair_initiator\":\n",
    "#         sentences_window[sentence] = sentences[k-window_width:k+window_width+1]\n",
    "\n",
    "# rep=pd.DataFrame(sentences_window)#.transpose()\n",
    "# repair_tok = list(read_corpus(rep.iloc[:,0], tokens_only=True))\n",
    "\n",
    "# inf_vecs = []\n",
    "# [inf_vecs.append(model.infer_vector(ut)) for ut in repair_tok]  \n",
    "\n",
    "# threshold = 0.2\n",
    "# paraphrasing = []\n",
    "\n",
    "# for e in range(len(inf_vecs)):\n",
    "#     counter = e+1\n",
    "#     while counter < len(inf_vecs):\n",
    "#         pair_sim = pairwise.cosine_similarity(inf_vecs[e].reshape(1, -1), inf_vecs[counter].reshape(1, -1))\n",
    "#         print(e, counter, pair_sim)\n",
    "#         if pair_sim >= threshold:\n",
    "#             print('YES', str(e), str(counter), str(pair_sim))\n",
    "#             paraphrasing.append((e,counter,pair_sim))\n",
    "#         counter +=1\n",
    "\n",
    "# paraphrasing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
