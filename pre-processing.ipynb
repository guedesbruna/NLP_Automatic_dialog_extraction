{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de9d64ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.io.json import json_normalize\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import sklearn\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e943a33b",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "\n",
    "## All preprocessing just needs to be done once. \n",
    "### If already done go direclty to DA patterns to load csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ccba6b",
   "metadata": {},
   "source": [
    "#### Open all datasets from different tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e7d0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t1 = pd.read_json('./data_TM2/flights.json')\n",
    "df_t2 = pd.read_json('./data_TM2/food-ordering.json')\n",
    "df_t3 = pd.read_json('./data_TM2/hotels.json')\n",
    "df_t4 = pd.read_json('./data_TM2/movies.json')\n",
    "df_t5 = pd.read_json('./data_TM2/music.json')\n",
    "df_t6 = pd.read_json('./data_TM2/restaurant-search.json')\n",
    "df_t7 = pd.read_json('./data_TM2/sports.json')\n",
    "\n",
    "# create extra column in the beginning for type of task before merging\n",
    "df_t1.insert(0, 'task', 'flights')\n",
    "df_t2.insert(0, 'task', 'food-ordering')\n",
    "df_t3.insert(0, 'task', 'hotels')\n",
    "df_t4.insert(0, 'task', 'movies')\n",
    "df_t5.insert(0, 'task', 'music')\n",
    "df_t6.insert(0, 'task', 'restaurant-search')\n",
    "df_t7.insert(0, 'task', 'sports')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7cfaa8",
   "metadata": {},
   "source": [
    "#### Normalize json and change to pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867f1f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_raw(df): #, name):\n",
    "    dialogs = []\n",
    "\n",
    "    for e,i in enumerate(df['utterances']):\n",
    "        for j in i:\n",
    "            new_df=pd.json_normalize(j)\n",
    "            new_df.insert(0, 'task', df['task'][e])\n",
    "            new_df.insert(1, 'conversation_id', df['conversation_id'][e])\n",
    "            new_df.insert(2, 'instruction_id', df['instruction_id'][e])\n",
    "        \n",
    "            dialogs.append(new_df)\n",
    "\n",
    "    large_df = pd.concat(dialogs, ignore_index=True)\n",
    "#   large_df.to_csv('./data_TM2/processed_utterances_'+name+'.csv')\n",
    "    \n",
    "    return large_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43582db7",
   "metadata": {},
   "source": [
    "#### Create unique dataset with normalized tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607586e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(df_list):\n",
    "    large_frames = []\n",
    "\n",
    "    for df_ in df_list:\n",
    "        large_df = process_raw(df_)\n",
    "        large_frames.append(large_df)\n",
    "\n",
    "    all_tasks = pd.concat(large_frames, ignore_index=True)\n",
    "    all_tasks.to_csv('./data_TM2/concatenated_tasks.csv')\n",
    "    \n",
    "    return all_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03061b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [df_t1, df_t2, df_t3, df_t4, df_t5, df_t6, df_t7]\n",
    "all_tasks = unique(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb3bd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #essa parte Ã© de antes porem com split incluindo simbolo do split\n",
    "# new_text = []\n",
    "# for ut in df['text']:\n",
    "#     new_ut = re.split('(\\.)', ut)\n",
    "#     new_text.append(new_ut)\n",
    "# df['new_text'] = new_text\n",
    "\n",
    "# #to keep delimiter in sentence\n",
    "# #not very efficient way of doing it, apparently a better way using regex symbols\n",
    "\n",
    "# new_lista = []\n",
    "# for row in df['new_text']:\n",
    "# #     print(row)\n",
    "#     for e, word in enumerate(row):\n",
    "#         if word == '.':\n",
    "#             new_lista.append(row[e-1]+row[e])\n",
    "#             new_lista.remove(row[e-1])\n",
    "#         else:\n",
    "#             new_lista.append(word)\n",
    "\n",
    "# df = df.explode('new_text')\n",
    "# df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3bf22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "df = pd.read_csv('./data_TM2/concatenated_tasks.csv', index_col=0)\n",
    "\n",
    "tokenized = []\n",
    "for ut in df['text']:\n",
    "    tokenized.append(sent_tokenize(ut))\n",
    "    \n",
    "df['new_lista'] = tokenized\n",
    "df = df.explode('new_lista')\n",
    "df = df.reset_index(drop=True)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890e096c",
   "metadata": {},
   "source": [
    "## Sample a part of the dataset to annotate\n",
    "### criteria:\n",
    "\n",
    "- take from all taks\n",
    "- annotate full dialogues\n",
    "- how many dialogues are there? what's the percentage of full dialogues i want to annotate?\n",
    "17304 dialogues (index 0 demarks the begining of a dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db5b2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of dialogues:\n",
    "df_count = pd.read_csv('./data_TM2/concatenated_tasks.csv', index_col=0)\n",
    "print(df_count['conversation_id'].nunique())\n",
    "\n",
    "#average size of dialog\n",
    "print(df_count['index'].mean())\n",
    "\n",
    "#dialogues per task (i.e True column)\n",
    "pd.crosstab(df_count['task'], df_count['index'] ==0).sort_values(by='task', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e87bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample randomly 167 dialogues, that will be split into 2 batches:\n",
    "#selection made using the unique conversation_ids\n",
    "\n",
    "np.random.seed(0)\n",
    "unique_conv_id = df['conversation_id'].unique()\n",
    "sample_ids = np.random.choice(unique_conv_id, 172, replace=False)\n",
    "\n",
    "sample_b1  = []\n",
    "sample_b2  = []\n",
    "for e, row in enumerate(df['conversation_id']):\n",
    "    for b1 in sample_ids[:int(len(sample_ids)/2)]:\n",
    "        if row == b1:\n",
    "            sample_b1.append(df.loc[e,:])\n",
    "    for b2 in sample_ids[int(len(sample_ids)/2):]:\n",
    "        if row == b2:\n",
    "            sample_b2.append(df.loc[e,:])\n",
    "\n",
    "df_sample_b1 = pd.DataFrame(sample_b1)\n",
    "df_sample_b2 = pd.DataFrame(sample_b2)\n",
    "\n",
    "df_sample_b1.to_csv('./data_TM2/sample_b1.csv')\n",
    "df_sample_b2.to_csv('./data_TM2/sample_b2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9b2f80",
   "metadata": {},
   "source": [
    "# Load gold label dataset to work with DA patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "453fdeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#format golden label to compare\n",
    "#then process automatic labels using this file\n",
    "\n",
    "df = pd.read_csv('./data_TM2/sample/sample_b1_goldannotated_noAutomaticLabels.csv', index_col=0)\n",
    "# df = df.iloc[:2034]\n",
    "df = df.drop(columns='G_DA_rep')\n",
    "\n",
    "gold = []\n",
    "for label in df['gold_labels']:\n",
    "    gold.append(str(label).split(','))\n",
    "\n",
    "df['gold_formatted'] = gold\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c5b320a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of dialogues sampled: 70\n",
      "number of utterances sampled: 2062\n",
      "number of tasks sampled: 7\n"
     ]
    }
   ],
   "source": [
    "#about the sample:\n",
    "\n",
    "print('number of dialogues sampled: ' + str(df['conversation_id'].nunique()))\n",
    "print('number of utterances sampled: ' + str(len(df)))\n",
    "print('number of tasks sampled: '+ str(df['task'].nunique()))\n",
    "# df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478c38d6",
   "metadata": {},
   "source": [
    "# OR Load whole dataset to work with DA patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c635da6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data_TM2/processed_utterances_all_tasks.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b7d4de",
   "metadata": {},
   "source": [
    "### Find substrings\n",
    "make list of words and rules for labeling DA\n",
    "https://towardsdatascience.com/check-for-a-substring-in-a-pandas-dataframe-column-4b949f64852#:~:text=Using%20%E2%80%9Ccontains%E2%80%9D%20to%20Find%20a,substring%20and%20False%20if%20not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4920aabb",
   "metadata": {},
   "source": [
    "- add new column for subset. since each utterance can only be one thing one column for all DA should be enough. \n",
    "- Later I'll have to check the vocabulary used for generic rules (FS_base_greeting or whatever is)\n",
    "- Also study in the UX book what is repair and other patterns:\n",
    "    - page 243 to 246: NCF pattern language summary\n",
    "\n",
    "Tips:\n",
    "- #NOT_INCLUDE_WORD: \n",
    "    - not_fifa = football_soccer_games.loc[~football_soccer_games['Name'].str.contains('FIFA')]\n",
    "- #to make a new dataframe with slicing subset:\n",
    "    - new_df = df.loc[df['text'].str.contains('|'.join(end_of_request), case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51656f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DA indicators. Obs: the vars bellow are sensitive to spacing !!! 'hi' != ' hi'\n",
    "\n",
    "# greeting = [' hi','hi.','hello','bye'] \n",
    "# repair_initiator = ['sorry','repeat', 'understand', 'mean'] \n",
    "# request_summary = ['correct?','confirm'] #when you ask an information to be confirmed\n",
    "# confirmation = ['yes','correct.','correct ', 'that\\'s it', 'indeed'] #confirmation Ã© a resposta que confirma: ex yes | correct | that's it\n",
    "# sentence_closer = ['awesome','great','perfect','exactly','that\\'s all','that is all', 'thanks', 'thank you', 'pleasure', 'excellent'] \n",
    "# question = ['\\?']\n",
    "# receipt = ['You are welcome', 'you\\'re welcome']\n",
    "# grant = ['got it', 'sure']\n",
    "\n",
    "# ## !!!!! order-sensitive > hierarchical!!!! this variable chooses only one label, rules goes on order. once it assigns one rule it won't check next\n",
    "\n",
    "# temp2=df.new_text.fillna(\"0\")\n",
    "# df['DA_indicators_sent'] =  np.where(temp2.str.contains('|'.join(repair_initiator), case=False), \"repair_initiator\",\n",
    "#                             np.where(temp2.str.contains('|'.join(greeting), case=False), \"greeting\",\n",
    "#                             np.where(temp2.str.contains('|'.join(request_summary), case=False),  \"request_summary\",\n",
    "#                             np.where(temp2.str.contains('|'.join(confirmation), case=False),  \"confirmation\",\n",
    "#                             np.where(temp2.str.contains('|'.join(receipt), case=False),  \"receipt\",\n",
    "#                             np.where(temp2.str.contains('|'.join(grant), case=False),  \"grant\",\n",
    "#                             np.where(temp2.str.contains('|'.join(question), case=False),  \"question\",\n",
    "#                             np.where(temp2.str.contains('|'.join(sentence_closer), case=False), \"sentence_closer\", \"x\"))))))))\n",
    "\n",
    "# df['DA_indicators_sent'].value_counts()\n",
    "\n",
    "# #it works! #binary variables and in the end gather in one new column\n",
    "\n",
    "# df['DA_rep'] =  np.where(temp2.str.contains('|'.join(repair_initiator), case=False), \"repair_initiator\", '')\n",
    "# df['DA_greet'] =  np.where(temp2.str.contains('|'.join(greeting), case=False), \"greeting\", '')\n",
    "# df['DA_req_sum'] =  np.where(temp2.str.contains('|'.join(request_summary), case=False), \"request_summary\", '')\n",
    "# df['DA_conf'] =  np.where(temp2.str.contains('|'.join(confirmation), case=False), \"confirmation\", '')\n",
    "# df['DA_receipt'] =  np.where(temp2.str.contains('|'.join(receipt), case=False), \"receipt\", '')\n",
    "# df['DA_grant'] =  np.where(temp2.str.contains('|'.join(grant), case=False), \"grant\", '')\n",
    "# df['DA_quest'] =  np.where(temp2.str.contains('|'.join(question), case=False), \"question\", '')\n",
    "# df['DA_closer'] =  np.where(temp2.str.contains('|'.join(sentence_closer), case=False), \"sentence_closer\", '')\n",
    "# df['all_DA'] = df[['DA_rep', 'DA_greet','DA_req_sum', 'DA_conf', 'DA_receipt', 'DA_grant', 'DA_quest', 'DA_closer']].agg(' '.join, axis=1)\n",
    "\n",
    "# df['all_DA'] = [word_tokenize(da) for da in df['all_DA']]\n",
    "\n",
    "# # df.loc[:, ['speaker', 'new_text', 'all_DA']].to_csv('analysis_multiple_DA_cases.csv')\n",
    "# df.loc[:, ['speaker', 'new_text', 'all_DA']]\n",
    "# counts = df['all_DA'].value_counts()\n",
    "\n",
    "# # counts.to_csv('count_overlaping_DAs.csv')\n",
    "# data = {'counts':counts}\n",
    "# df_counts = pd.DataFrame(data)\n",
    "# df_counts.head(45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f163b77",
   "metadata": {},
   "source": [
    "## Second iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65652e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DA indicators. Obs: the vars bellow are sensitive to spacing !!! 'hi' != ' hi'\n",
    "\n",
    "# greeting = [' hi','hi.','hello','yo ', 'hey'] \n",
    "# repair_initiator = ['sorry','repeat', 'understand', 'mean ', 'what?', 'example?']\n",
    "# repair = ['I meant']\n",
    "# request_summary = [' correct?','confirm']  \n",
    "# confirmation = ['yes',' correct.',' correct ', 'that\\'s it', 'indeed', 'yeah', 'that\\'s right', 'you got it', 'yep', 'sure', 'okay', 'all right'] \n",
    "# sequence_closer = ['awesome','great','perfect','exactly','that\\'s all','that is all', 'thanks', 'thank you', 'pleasure', 'excellent', 'okay', 'excellent', 'too bad', 'oh well']\n",
    "# inquiry = ['\\?']\n",
    "# receipt = ['You are welcome', 'you\\'re welcome']\n",
    "# disconfirmation = [' no ', 'wrong', 'incorrect', 'not really']\n",
    "\n",
    "# ## !!!!! order-sensitive !!!!\n",
    "\n",
    "# #this variable chooses only one label, rules goes on order. once it assigns one rule it won't check next\n",
    "# temp2=df.new_text.fillna(\"0\")\n",
    "# df['DA_indicators_sent'] =  np.where(temp2.str.contains('|'.join(repair_initiator), case=False), \"repair_initiator\",\n",
    "#                             np.where(temp2.str.contains('|'.join(greeting), case=False), \"greeting\",\n",
    "#                             np.where(temp2.str.contains('|'.join(request_summary), case=False),  \"request_summary\",\n",
    "#                             np.where(temp2.str.contains('|'.join(confirmation), case=False),  \"confirmation\",\n",
    "#                             np.where(temp2.str.contains('|'.join(receipt), case=False),  \"receipt\",\n",
    "#                             np.where(temp2.str.contains('|'.join(disconfirmation), case=False),  \"disconfirmation\",\n",
    "#                             np.where(temp2.str.contains('|'.join(inquiry), case=False),  \"inquiry\",\n",
    "#                             np.where(temp2.str.contains('|'.join(repair), case=False),  \"repair\",\n",
    "#                             np.where(temp2.str.contains('|'.join(sequence_closer), case=False), \"sequence_closer\", \"x\")))))))))\n",
    "\n",
    "# print(df['DA_indicators_sent'].value_counts())\n",
    "\n",
    "# #it works! #binary variables and in the end gather in one new column\n",
    "# ##### ainda n sei se repair solo fica ######                                     \n",
    "                                     \n",
    "# df['DA_rep_init'] =  np.where(temp2.str.contains('|'.join(repair_initiator), case=False), \"repair_initiator\", '')\n",
    "# df['DA_rep'] =  np.where(temp2.str.contains('|'.join(repair), case=False), \"repair\", '')\n",
    "# df['DA_greet'] =  np.where(temp2.str.contains('|'.join(greeting), case=False), \"greeting\", '')\n",
    "# df['DA_req_sum'] =  np.where(temp2.str.contains('|'.join(request_summary), case=False), \"request_summary\", '')\n",
    "# df['DA_conf'] =  np.where(temp2.str.contains('|'.join(confirmation), case=False), \"confirmation\", '')\n",
    "# df['DA_receipt'] =  np.where(temp2.str.contains('|'.join(receipt), case=False), \"receipt\", '')\n",
    "# df['DA_disconf'] =  np.where(temp2.str.contains('|'.join(disconfirmation), case=False), \"disconfirmation\", '')\n",
    "# df['DA_inquiry'] =  np.where(temp2.str.contains('|'.join(inquiry), case=False), \"inquiry\", '')\n",
    "# df['DA_closer'] =  np.where(temp2.str.contains('|'.join(sequence_closer), case=False), \"sequence_closer\", '')\n",
    "# df['all_DA'] = df[['DA_rep_init', 'DA_rep', 'DA_greet','DA_req_sum', 'DA_conf', 'DA_receipt', 'DA_disconf', 'DA_inquiry', 'DA_closer']].agg(' '.join, axis=1)\n",
    "\n",
    "# df['all_DA'] = [word_tokenize(da) for da in df['all_DA']]\n",
    "\n",
    "# df.loc[:, ['speaker', 'new_text', 'all_DA']].to_csv('analysis_multiple_DA_cases_2nditeration.csv')\n",
    "# df.loc[:, ['speaker', 'new_text', 'all_DA']]\n",
    "# counts = df['all_DA'].value_counts()\n",
    "\n",
    "# counts.to_csv('count_overlaping_DAs_2nditeration.csv')\n",
    "# data = {'counts':counts}\n",
    "# df_counts = pd.DataFrame(data)\n",
    "# df_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea1b246",
   "metadata": {},
   "source": [
    "## Third iteration and FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49963883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DA indicators. Obs: the vars bellow are sensitive to spacing !!! 'hi' != ' hi'\n",
    "\n",
    "greeting = [' hi','hi.','hello','yo ', 'hey', 'How can I help you?'] \n",
    "# repair_initiator = ['sorry','repeat', 'understand', 'mean?', 'what?', 'example?', 'could you say that again?', 'say again', 'what did you say?', 'I can\\'t hear you', 'I didn\\'t listen', 'say it again']\n",
    "\n",
    "repair_initiator = ['Could you please repeat','Can you repeat','Could you say that again?','say again','what did you say','what else did you say?','I can\\'t hear you.','repeat please','you said','sorry?','what do you mean?']\n",
    "# repair = ['I meant']\n",
    "request_summary = [' correct?','confirm']  \n",
    "confirmation = ['yes',' correct.',' correct ', 'that\\'s it', 'indeed', 'yeah', 'that\\'s right', 'you got it', 'yep', 'sure', 'okay', 'all right',  'sure', 'sounds good', 'super.', 'super!', 'alright', 'I don\\'t mind', 'I can help you', 'sounds really good', 'got it.'] \n",
    "sequence_closer = ['awesome','great','perfect','exactly','that\\'s all','that is all', 'thanks', 'thank you', 'pleasure', 'excellent', 'okay', 'excellent', 'too bad', 'oh well', 'have a good day', 'enjoy', 'until next time', 'good day', 'good luck', 'bye', 'til next time']\n",
    "receipt = ['You are welcome', 'you\\'re welcome']\n",
    "disconfirmation = [' no ', 'wrong', 'incorrect', 'not really']\n",
    "#NEW\n",
    "completion_check = ['is that all?','anything else I can help you with?', 'anything else?'] \n",
    "partial_request = ['\\?']\n",
    "detail_request = ['\\?']\n",
    "hold_request = ['one moment, please',' hold on', 'hold', 'just a moment', 'let me check for you', 'let me see', 'one sec' ]\n",
    "grant_answer = ['.']\n",
    "temp2=df.new_text.fillna(\"0\")\n",
    "\n",
    "## !!!!! order-sensitive !!!!\n",
    "\n",
    "# #this variable chooses only one label, rules goes on order. once it assigns one rule it won't check next\n",
    "\n",
    "# df['DA_indicators_sent'] =  np.where(temp2.str.contains('|'.join(repair_initiator), case=False), \"repair_initiator\",\n",
    "#                             np.where(temp2.str.contains('|'.join(greeting), case=False), \"greeting\",\n",
    "#                             np.where(temp2.str.contains('|'.join(request_summary), case=False),  \"request_summary\",\n",
    "#                             np.where(temp2.str.contains('|'.join(confirmation), case=False),  \"confirmation\",\n",
    "#                             np.where(temp2.str.contains('|'.join(receipt), case=False),  \"receipt\",\n",
    "#                             np.where(temp2.str.contains('|'.join(disconfirmation), case=False),  \"disconfirmation\",\n",
    "#                             np.where(temp2.str.contains('|'.join(repair), case=False),  \"repair\",\n",
    "#                             np.where(temp2.str.contains('|'.join(sequence_closer), case=False), \"sequence_closer\", \n",
    "#                             np.where(temp2.str.contains('|'.join(completion_check), case=False),  \"completion_check\",\n",
    "#                             np.where(temp2.str.contains('|'.join(hold_request), case=False),  \"hold_request\",\n",
    "#                             np.where(temp2.str.contains('|'.join(partial_request), case=False),  \"partial_request\",\n",
    "#                             np.where(temp2.str.contains('|'.join(detail_request), case=False),  \"detail_request\",\"x\"))))))))))))\n",
    "\n",
    "# print(df['DA_indicators_sent'].value_counts())\n",
    "\n",
    "\n",
    "#it works! #binary variables and in the end gather in one new column\n",
    "##### ainda n sei se repair solo fica ######                                     \n",
    "                                     \n",
    "df['DA_rep_init'] =  np.where(temp2.str.contains('|'.join(repair_initiator), case=False), \"repair_initiator\", '')\n",
    "# df['DA_rep'] =  np.where(temp2.str.contains('|'.join(repair), case=False), \"repair\", '')\n",
    "df['DA_greet'] =  np.where(temp2.str.contains('|'.join(greeting), case=False), \"greeting\", '')\n",
    "df['DA_req_sum'] =  np.where(temp2.str.contains('|'.join(request_summary), case=False), \"request_summary\", '')\n",
    "df['DA_conf'] =  np.where(temp2.str.contains('|'.join(confirmation), case=False), \"confirmation\", '')\n",
    "df['DA_receipt'] =  np.where(temp2.str.contains('|'.join(receipt), case=False), \"receipt\", '')\n",
    "df['DA_disconf'] =  np.where(temp2.str.contains('|'.join(disconfirmation), case=False), \"disconfirmation\", '')\n",
    "df['DA_closer'] =  np.where(temp2.str.contains('|'.join(sequence_closer), case=False), \"sequence_closer\", '')           \n",
    "df['DA_comp_check'] =  np.where(temp2.str.contains('|'.join(completion_check), case=False),  \"completion_check\", '')\n",
    "df['DA_hold'] =  np.where(temp2.str.contains('|'.join(hold_request), case=False),  \"hold_request\", '')\n",
    "df['DA_partial_req'] = np.where(((df['speaker'] == 'USER') & temp2.str.contains('|'.join(partial_request), case=False)), 'partial_request', '')\n",
    "df['DA_detail_req'] = np.where(((df['speaker'] == 'ASSISTANT') & temp2.str.contains('|'.join(detail_request), case=False)), 'detail_request', '')\n",
    "df['DA_grant'] = np.where(((df['speaker'] == 'ASSISTANT') & (temp2.str.contains('|'.join(grant_answer), case=False) & (df['DA_partial_req'].shift(1) == 'partial_request') )), 'grant', '')                                     \n",
    "df['DA_answer'] = np.where(((df['speaker'] == 'USER') & (temp2.str.contains('|'.join(grant_answer), case=False) & (df['DA_detail_req'].shift(1) == 'detail_request') )), 'answer', '')                                     \n",
    "\n",
    "\n",
    "df['all_DA'] = df[['DA_rep_init', 'DA_greet','DA_req_sum', 'DA_conf', 'DA_receipt', 'DA_disconf', 'DA_closer', 'DA_comp_check', 'DA_hold', 'DA_partial_req', 'DA_detail_req', 'DA_grant', 'DA_answer']].agg(' '.join, axis=1)\n",
    "\n",
    "df['all_DA'] = [word_tokenize(da) for da in df['all_DA']]\n",
    "\n",
    "# df.loc[:, ['speaker', 'new_text', 'all_DA']].to_csv('analysis_multiple_DA_cases_2nditeration.csv')\n",
    "df.loc[:, ['speaker', 'new_text', 'all_DA']]\n",
    "counts = df['all_DA'].value_counts()\n",
    "\n",
    "# counts.to_csv('count_overlaping_DAs_3iteration.csv')\n",
    "data = {'counts':counts}\n",
    "df_counts = pd.DataFrame(data)\n",
    "# df_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c09997e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of utterances without label: 488\n",
      "total number of utterances in selected dataset: 2062\n",
      "percentage of data synthetically labeled: 0.7633365664403492\n"
     ]
    }
   ],
   "source": [
    "labeled_ut_count = df[\"all_DA\"].apply(lambda x: 1 if len(x) == 0 else 0).sum()\n",
    "        \n",
    "print('number of utterances without label: '+ str(labeled_ut_count))\n",
    "print('total number of utterances in selected dataset: ' + str(len(df['all_DA'])))\n",
    "print('percentage of data synthetically labeled: ' + str(1- (labeled_ut_count/len(df['all_DA']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0f257f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data_TM2/GOLD_and_SYNT_annotated_data_.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d349af72",
   "metadata": {},
   "source": [
    "## Compare binary variable against binary. \n",
    "# !! only if working with manually annotated dataset. for whole dataset skip this!!\n",
    "## Kappa score\n",
    "#### I think it makes no sense to compare the multiple labels together because if only one is different it gives a zero for all\n",
    "\n",
    "The kappa statistic, which is a number between -1 and 1. The maximum value means complete agreement; zero or lower means chance agreement.\n",
    "\n",
    "Conclusion: remove request summary, examples are not matching. Also I don't see any easy to implement rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f39e724e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G_DA_grant\n",
      "DA_grant\n"
     ]
    }
   ],
   "source": [
    "#sanity check that columns of gold and synt are aligned:\n",
    "\n",
    "print(df.columns[20])\n",
    "print(df.columns[20+14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f435c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.iloc[:,[9,23]]\n",
    "\n",
    "DA = ['repair_initiator','greeting','request_summary','confirmation','receipt','disconfirmation',\n",
    "      'sequence_closer','completion_check','hold_request','partial_request', 'detail_request', 'grant', 'answer'] \n",
    "kappa = []\n",
    "prec_rec_f1 = []\n",
    "count_g = []\n",
    "count_s = []\n",
    "\n",
    "for e,i in enumerate(range(9,22)):\n",
    "    gold = np.asarray([0 if val != DA[e] else 1 for val in df.iloc[:, i]])\n",
    "    synt = np.asarray([0 if val != DA[e] else 1 for val in df.iloc[:, (i+14)]])\n",
    "    kappa.append(sklearn.metrics.cohen_kappa_score(gold, synt))\n",
    "    prec_rec_f1.append(precision_recall_fscore_support(gold, synt, average='macro', zero_division=0))\n",
    "    unique_g, counts_g = np.unique(gold, return_counts=True)\n",
    "    unique_s, counts_s = np.unique(synt, return_counts=True)\n",
    "    count_g.append(counts_g[1])\n",
    "    count_s.append(counts_s[1])\n",
    "\n",
    "# print('kappa: ' + str(np.mean(kappa)))  \n",
    "# kappa_scores = list(zip(DA,kappa))\n",
    "\n",
    "kappa_scores = pd.DataFrame(kappa, index = DA, columns = ['Kappa'])\n",
    "# print(kappa_scores)\n",
    "\n",
    "metrics = pd.DataFrame(prec_rec_f1, index = DA, columns = ['Precision','Recall','F1_macro','Support'])\n",
    "metrics = metrics.drop(columns = 'Support')\n",
    "# metrics = metrics.insert(loc=0,column=count_g, value='Count_Gold')\n",
    "metrics['Cohen\\'s Kappa'] = kappa_scores\n",
    "metrics['Count Gold'] = count_g\n",
    "metrics['Count Synt'] = count_s\n",
    "metrics.sort_values(by='F1_macro', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2331b7",
   "metadata": {},
   "source": [
    "## Getting insights from k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dbb51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df['new_text'])\n",
    "\n",
    "true_k = 5\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "for i in range(true_k):\n",
    "    print('Cluster %d:' % i),\n",
    "    for ind in order_centroids[i, :5]:\n",
    "        print(' %s' % terms[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df3fdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Prediction')\n",
    "X = vectorizer.transform(['hi, can you help me?'])\n",
    "predicted = model.predict(X)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d5e6ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
