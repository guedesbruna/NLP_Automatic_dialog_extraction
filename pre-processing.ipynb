{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e943a33b",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "\n",
    "## All preprocessing just needs to be done once. \n",
    "### If already done go direclty to DA patterns to load csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de9d64ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.io.json import json_normalize\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ccba6b",
   "metadata": {},
   "source": [
    "#### Open all datasets from different tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e7d0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t1 = pd.read_json('./data_TM2/flights.json')\n",
    "df_t2 = pd.read_json('./data_TM2/food-ordering.json')\n",
    "df_t3 = pd.read_json('./data_TM2/hotels.json')\n",
    "df_t4 = pd.read_json('./data_TM2/movies.json')\n",
    "df_t5 = pd.read_json('./data_TM2/music.json')\n",
    "df_t6 = pd.read_json('./data_TM2/restaurant-search.json')\n",
    "df_t7 = pd.read_json('./data_TM2/sports.json')\n",
    "\n",
    "# create extra column in the beginning for type of task before merging\n",
    "df_t1.insert(0, 'task', 'flights')\n",
    "df_t2.insert(0, 'task', 'food-ordering')\n",
    "df_t3.insert(0, 'task', 'hotels')\n",
    "df_t4.insert(0, 'task', 'movies')\n",
    "df_t5.insert(0, 'task', 'music')\n",
    "df_t6.insert(0, 'task', 'restaurant-search')\n",
    "df_t7.insert(0, 'task', 'sports')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7cfaa8",
   "metadata": {},
   "source": [
    "#### Normalize json and change to pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867f1f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_raw(df): #, name):\n",
    "    dialogs = []\n",
    "\n",
    "    for e,i in enumerate(df['utterances']):\n",
    "        for j in i:\n",
    "            new_df=pd.json_normalize(j)\n",
    "            new_df.insert(0, 'task', df['task'][e])\n",
    "            new_df.insert(1, 'conversation_id', df['conversation_id'][e])\n",
    "            new_df.insert(2, 'instruction_id', df['instruction_id'][e])\n",
    "        \n",
    "            dialogs.append(new_df)\n",
    "\n",
    "    large_df = pd.concat(dialogs, ignore_index=True)\n",
    "#   large_df.to_csv('./data_TM2/processed_utterances_'+name+'.csv')\n",
    "    \n",
    "    return large_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43582db7",
   "metadata": {},
   "source": [
    "#### Create unique dataset with normalized tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607586e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(df_list):\n",
    "    large_frames = []\n",
    "\n",
    "    for df_ in df_list:\n",
    "        large_df = process_raw(df_)\n",
    "        large_frames.append(large_df)\n",
    "\n",
    "    all_tasks = pd.concat(large_frames, ignore_index=True)\n",
    "    all_tasks.to_csv('./data_TM2/concatenated_tasks.csv')\n",
    "    \n",
    "    return all_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03061b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [df_t1, df_t2, df_t3, df_t4, df_t5, df_t6, df_t7]\n",
    "all_tasks = unique(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb3bd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #essa parte é de antes porem com split incluindo simbolo do split\n",
    "# new_text = []\n",
    "# for ut in df['text']:\n",
    "#     new_ut = re.split('(\\.)', ut)\n",
    "#     new_text.append(new_ut)\n",
    "# df['new_text'] = new_text\n",
    "\n",
    "# #to keep delimiter in sentence\n",
    "# #not very efficient way of doing it, apparently a better way using regex symbols\n",
    "\n",
    "# new_lista = []\n",
    "# for row in df['new_text']:\n",
    "# #     print(row)\n",
    "#     for e, word in enumerate(row):\n",
    "#         if word == '.':\n",
    "#             new_lista.append(row[e-1]+row[e])\n",
    "#             new_lista.remove(row[e-1])\n",
    "#         else:\n",
    "#             new_lista.append(word)\n",
    "\n",
    "# df = df.explode('new_text')\n",
    "# df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3bf22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "df = pd.read_csv('./data_TM2/concatenated_tasks.csv', index_col=0)\n",
    "\n",
    "tokenized = []\n",
    "for ut in df['text']:\n",
    "    tokenized.append(sent_tokenize(ut))\n",
    "    \n",
    "df['new_lista'] = tokenized\n",
    "df = df.explode('new_lista')\n",
    "df = df.reset_index(drop=True)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9b2f80",
   "metadata": {},
   "source": [
    "## Load whole dataset to work with DA patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c635da6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data_TM2/processed_utterances_all_tasks.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b7d4de",
   "metadata": {},
   "source": [
    "### Find substrings\n",
    "make list of words and rules for labeling DA\n",
    "https://towardsdatascience.com/check-for-a-substring-in-a-pandas-dataframe-column-4b949f64852#:~:text=Using%20%E2%80%9Ccontains%E2%80%9D%20to%20Find%20a,substring%20and%20False%20if%20not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4920aabb",
   "metadata": {},
   "source": [
    "- add new column for subset. since each utterance can only be one thing one column for all DA should be enough. \n",
    "- Later I'll have to check the vocabulary used for generic rules (FS_base_greeting or whatever is)\n",
    "- Also study in the UX book what is repair and other patterns:\n",
    "    - page 243 to 246: NCF pattern language summary\n",
    "\n",
    "Tips:\n",
    "- #NOT_INCLUDE_WORD: \n",
    "    - not_fifa = football_soccer_games.loc[~football_soccer_games['Name'].str.contains('FIFA')]\n",
    "- #to make a new dataframe with slicing subset:\n",
    "    - new_df = df.loc[df['text'].str.contains('|'.join(end_of_request), case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51656f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DA indicators. Obs: the vars bellow are sensitive to spacing !!! 'hi' != ' hi'\n",
    "\n",
    "greeting = [' hi','hi.','hello','bye'] \n",
    "repair_initiator = ['sorry','repeat', 'understand', 'mean'] \n",
    "request_summary = ['correct?','confirm'] #when you ask an information to be confirmed\n",
    "confirmation = ['yes','correct.','correct ', 'that\\'s it', 'indeed'] #confirmation é a resposta que confirma: ex yes | correct | that's it\n",
    "sentence_closer = ['awesome','great','perfect','exactly','that\\'s all','that is all', 'thanks', 'thank you', 'pleasure', 'excellent'] \n",
    "question = ['\\?']\n",
    "receipt = ['You are welcome', 'you\\'re welcome']\n",
    "grant = ['got it', 'sure']\n",
    "\n",
    "## !!!!! order-sensitive > hierarchical!!!! this variable chooses only one label, rules goes on order. once it assigns one rule it won't check next\n",
    "\n",
    "temp2=df.new_text.fillna(\"0\")\n",
    "df['DA_indicators_sent'] =  np.where(temp2.str.contains('|'.join(repair_initiator), case=False), \"repair_initiator\",\n",
    "                            np.where(temp2.str.contains('|'.join(greeting), case=False), \"greeting\",\n",
    "                            np.where(temp2.str.contains('|'.join(request_summary), case=False),  \"request_summary\",\n",
    "                            np.where(temp2.str.contains('|'.join(confirmation), case=False),  \"confirmation\",\n",
    "                            np.where(temp2.str.contains('|'.join(receipt), case=False),  \"receipt\",\n",
    "                            np.where(temp2.str.contains('|'.join(grant), case=False),  \"grant\",\n",
    "                            np.where(temp2.str.contains('|'.join(question), case=False),  \"question\",\n",
    "                            np.where(temp2.str.contains('|'.join(sentence_closer), case=False), \"sentence_closer\", \"x\"))))))))\n",
    "\n",
    "df['DA_indicators_sent'].value_counts()\n",
    "\n",
    "#it works! #binary variables and in the end gather in one new column\n",
    "\n",
    "df['DA_rep'] =  np.where(temp2.str.contains('|'.join(repair_initiator), case=False), \"repair_initiator\", '')\n",
    "df['DA_greet'] =  np.where(temp2.str.contains('|'.join(greeting), case=False), \"greeting\", '')\n",
    "df['DA_req_sum'] =  np.where(temp2.str.contains('|'.join(request_summary), case=False), \"request_summary\", '')\n",
    "df['DA_conf'] =  np.where(temp2.str.contains('|'.join(confirmation), case=False), \"confirmation\", '')\n",
    "df['DA_receipt'] =  np.where(temp2.str.contains('|'.join(receipt), case=False), \"receipt\", '')\n",
    "df['DA_grant'] =  np.where(temp2.str.contains('|'.join(grant), case=False), \"grant\", '')\n",
    "df['DA_quest'] =  np.where(temp2.str.contains('|'.join(question), case=False), \"question\", '')\n",
    "df['DA_closer'] =  np.where(temp2.str.contains('|'.join(sentence_closer), case=False), \"sentence_closer\", '')\n",
    "df['all_DA'] = df[['DA_rep', 'DA_greet','DA_req_sum', 'DA_conf', 'DA_receipt', 'DA_grant', 'DA_quest', 'DA_closer']].agg(' '.join, axis=1)\n",
    "\n",
    "df['all_DA'] = [word_tokenize(da) for da in df['all_DA']]\n",
    "\n",
    "# df.loc[:, ['speaker', 'new_text', 'all_DA']].to_csv('analysis_multiple_DA_cases.csv')\n",
    "df.loc[:, ['speaker', 'new_text', 'all_DA']]\n",
    "counts = df['all_DA'].value_counts()\n",
    "\n",
    "# counts.to_csv('count_overlaping_DAs.csv')\n",
    "data = {'counts':counts}\n",
    "df_counts = pd.DataFrame(data)\n",
    "df_counts.head(45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f163b77",
   "metadata": {},
   "source": [
    "## Second iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a65652e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x                   215931\n",
      "greeting             80406\n",
      "confirmation         68747\n",
      "sequence_closer      33260\n",
      "repair_initiator     30590\n",
      "receipt               4380\n",
      "request_summary       2233\n",
      "disconfirmation       1324\n",
      "repair                  12\n",
      "Name: DA_indicators_sent, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>[]</th>\n",
       "      <td>166900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[inquiry]</th>\n",
       "      <td>49031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[greeting]</th>\n",
       "      <td>46156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[confirmation, sequence_closer]</th>\n",
       "      <td>39767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[sequence_closer]</th>\n",
       "      <td>32785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 counts\n",
       "[]                               166900\n",
       "[inquiry]                         49031\n",
       "[greeting]                        46156\n",
       "[confirmation, sequence_closer]   39767\n",
       "[sequence_closer]                 32785"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DA indicators. Obs: the vars bellow are sensitive to spacing !!! 'hi' != ' hi'\n",
    "\n",
    "greeting = [' hi','hi.','hello','yo ', 'hey'] \n",
    "repair_initiator = ['sorry','repeat', 'understand', 'mean ', 'what?', 'example?']\n",
    "repair = ['I meant']\n",
    "request_summary = [' correct?','confirm']  \n",
    "confirmation = ['yes',' correct.',' correct ', 'that\\'s it', 'indeed', 'yeah', 'that\\'s right', 'you got it', 'yep', 'sure', 'okay', 'all right'] \n",
    "sequence_closer = ['awesome','great','perfect','exactly','that\\'s all','that is all', 'thanks', 'thank you', 'pleasure', 'excellent', 'okay', 'excellent', 'too bad', 'oh well']\n",
    "inquiry = ['\\?']\n",
    "receipt = ['You are welcome', 'you\\'re welcome']\n",
    "disconfirmation = [' no ', 'wrong', 'incorrect', 'not really']\n",
    "\n",
    "## !!!!! order-sensitive !!!!\n",
    "\n",
    "#this variable chooses only one label, rules goes on order. once it assigns one rule it won't check next\n",
    "temp2=df.new_text.fillna(\"0\")\n",
    "df['DA_indicators_sent'] =  np.where(temp2.str.contains('|'.join(repair_initiator), case=False), \"repair_initiator\",\n",
    "                            np.where(temp2.str.contains('|'.join(greeting), case=False), \"greeting\",\n",
    "                            np.where(temp2.str.contains('|'.join(request_summary), case=False),  \"request_summary\",\n",
    "                            np.where(temp2.str.contains('|'.join(confirmation), case=False),  \"confirmation\",\n",
    "                            np.where(temp2.str.contains('|'.join(receipt), case=False),  \"receipt\",\n",
    "                            np.where(temp2.str.contains('|'.join(disconfirmation), case=False),  \"disconfirmation\",\n",
    "                            np.where(temp2.str.contains('|'.join(disconfirmation), case=False),  \"disconfirmation\",\n",
    "                            np.where(temp2.str.contains('|'.join(repair), case=False),  \"repair\",\n",
    "                            np.where(temp2.str.contains('|'.join(sequence_closer), case=False), \"sequence_closer\", \"x\")))))))))\n",
    "\n",
    "print(df['DA_indicators_sent'].value_counts())\n",
    "\n",
    "\n",
    "#it works! #binary variables and in the end gather in one new column\n",
    "##### ainda n sei se repair solo fica ######                                     \n",
    "                                     \n",
    "df['DA_rep_init'] =  np.where(temp2.str.contains('|'.join(repair_initiator), case=False), \"repair_initiator\", '')\n",
    "df['DA_rep'] =  np.where(temp2.str.contains('|'.join(repair), case=False), \"repair\", '')\n",
    "df['DA_greet'] =  np.where(temp2.str.contains('|'.join(greeting), case=False), \"greeting\", '')\n",
    "df['DA_req_sum'] =  np.where(temp2.str.contains('|'.join(request_summary), case=False), \"request_summary\", '')\n",
    "df['DA_conf'] =  np.where(temp2.str.contains('|'.join(confirmation), case=False), \"confirmation\", '')\n",
    "df['DA_receipt'] =  np.where(temp2.str.contains('|'.join(receipt), case=False), \"receipt\", '')\n",
    "df['DA_disconf'] =  np.where(temp2.str.contains('|'.join(disconfirmation), case=False), \"disconfirmation\", '')\n",
    "df['DA_inquiry'] =  np.where(temp2.str.contains('|'.join(inquiry), case=False), \"inquiry\", '')\n",
    "df['DA_closer'] =  np.where(temp2.str.contains('|'.join(sequence_closer), case=False), \"sequence_closer\", '')\n",
    "df['all_DA'] = df[['DA_rep_init', 'DA_rep', 'DA_greet','DA_req_sum', 'DA_conf', 'DA_receipt', 'DA_disconf', 'DA_inquiry', 'DA_closer']].agg(' '.join, axis=1)\n",
    "\n",
    "df['all_DA'] = [word_tokenize(da) for da in df['all_DA']]\n",
    "\n",
    "df.loc[:, ['speaker', 'new_text', 'all_DA']].to_csv('analysis_multiple_DA_cases_2nditeration.csv')\n",
    "df.loc[:, ['speaker', 'new_text', 'all_DA']]\n",
    "counts = df['all_DA'].value_counts()\n",
    "\n",
    "counts.to_csv('count_overlaping_DAs_2nditeration.csv')\n",
    "data = {'counts':counts}\n",
    "df_counts = pd.DataFrame(data)\n",
    "df_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890e096c",
   "metadata": {},
   "source": [
    "## Now sample a part to annotate\n",
    "### criteria:\n",
    "\n",
    "- take from all taks\n",
    "- annotate full dialogues\n",
    "- how many dialogues are there? what's the percentage of full dialogues i want to annotate?\n",
    "17304 dialogues (index 0 demarks the begining of a dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4db5b2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17289\n",
      "11.578471098680227\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>index</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sports</th>\n",
       "      <td>45099</td>\n",
       "      <td>3481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>restaurant-search</th>\n",
       "      <td>62929</td>\n",
       "      <td>3276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>music</th>\n",
       "      <td>25309</td>\n",
       "      <td>1603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movies</th>\n",
       "      <td>56701</td>\n",
       "      <td>3056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hotels</th>\n",
       "      <td>60691</td>\n",
       "      <td>2357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>food-ordering</th>\n",
       "      <td>12903</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flights</th>\n",
       "      <td>60865</td>\n",
       "      <td>2481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "index              False  True\n",
       "task                          \n",
       "sports             45099  3481\n",
       "restaurant-search  62929  3276\n",
       "music              25309  1603\n",
       "movies             56701  3056\n",
       "hotels             60691  2357\n",
       "food-ordering      12903  1050\n",
       "flights            60865  2481"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of dialogues:\n",
    "df_count = pd.read_csv('./data_TM2/concatenated_tasks.csv', index_col=0)\n",
    "print(df_count['conversation_id'].nunique())\n",
    "\n",
    "#average size of dialog\n",
    "print(df_count['index'].mean())\n",
    "\n",
    "#dialogues per task (i.e True column)\n",
    "pd.crosstab(df_count['task'], df_count['index'] ==0).sort_values(by='task', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ffbae9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample randomly 167 dialogues, that will be split into 2 batches:\n",
    "#selection made using the unique conversation_ids\n",
    "\n",
    "unique_conv_id = df['conversation_id'].unique()\n",
    "sample_ids = np.random.choice(unique_conv_id, 172)\n",
    "\n",
    "sample_b1  = []\n",
    "sample_b2  = []\n",
    "for e, row in enumerate(df['conversation_id']):\n",
    "    for b1 in sample_ids[:int(len(sample_ids)/2)]:\n",
    "        if row == b1:\n",
    "            sample_b1.append(df.loc[e,:])\n",
    "    for b2 in sample_ids[int(len(sample_ids)/2):]:\n",
    "        if row == b2:\n",
    "            sample_b2.append(df.loc[e,:])\n",
    "\n",
    "df_sample_b1 = pd.DataFrame(sample_b1)\n",
    "df_sample_b2 = pd.DataFrame(sample_b2)\n",
    "\n",
    "df_sample_b1.to_csv('./data_TM2/sample_b1.csv')\n",
    "df_sample_b2.to_csv('./data_TM2/sample_b2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2331b7",
   "metadata": {},
   "source": [
    "## Getting insights from k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dbb51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df['new_text'])\n",
    "\n",
    "true_k = 5\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "for i in range(true_k):\n",
    "    print('Cluster %d:' % i),\n",
    "    for ind in order_centroids[i, :5]:\n",
    "        print(' %s' % terms[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df3fdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Prediction')\n",
    "X = vectorizer.transform(['hi, can you help me?'])\n",
    "predicted = model.predict(X)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26edefe5",
   "metadata": {},
   "source": [
    "## Repair cases\n",
    "\n",
    "Next round to detecting misunderstanding repair happens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ca63d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS WORKS! need to apply all filters\n",
    "# df.loc[df['column name'] condition, 'new column name'] = 'value if condition is met'\n",
    "\n",
    "df.loc[(df['speaker'] != df['speaker'].shift(1)) & (df['DA_indicators_sent'].shift(1) == 'repair_initiator'), 'teste_col2'] = 'SECOND'\n",
    "\n",
    "df['teste_col2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7491b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['teste_col2'] == 'SECOND']\n",
    "#second is more an ANSWER (i.e. next round) to: sorry, what did you say? OR what did you mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1ddda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('a case of self repair: ')\n",
    "print(df.iloc[1708,4:-1])\n",
    "print('\\n\\n')\n",
    "print(df.iloc[1709,4:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d34113",
   "metadata": {},
   "source": [
    "## Cosine similarity for paraphrasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe2f8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= 'he is the king of the world'\n",
    "b= 'His majesty rules the planet'\n",
    "c= 'I love my dog'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09715687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
